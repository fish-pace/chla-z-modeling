{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5727d3b-fd67-47b9-9ba9-403fb96d8538",
   "metadata": {},
   "source": [
    "# Process daily files in dask gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b17c21-afb1-4c89-b272-cbffc07a75e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ml_utils' from '/home/jovyan/chla-z-modeling/ml_utils.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Core data handling and plotting libraries ---\n",
    "import earthaccess\n",
    "import xarray as xr       # for working with labeled multi-dimensional arrays\n",
    "import numpy as np        # for numerical operations on arrays\n",
    "import pandas as pd\n",
    "# --- Custom python functions ---\n",
    "import os, importlib\n",
    "# Looks to see if you have the file already and if not, downloads from GitHub\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    !wget -q https://raw.githubusercontent.com/fish-pace/2025-tutorials/main/ml_utils.py\n",
    "\n",
    "import ml_utils as mu\n",
    "importlib.reload(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1e3d3a-cb19-4201-91b4-98aefc16479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_granule(\n",
    "    res,\n",
    "    lat_chunk=100,\n",
    "    lon_chunk=100,\n",
    "    bucket_name=\"nmfs_odp_nwfsc\",\n",
    "    destination_prefix=\"CB/fish-pace-datasets/chla-z/netcdf\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the full pipeline for a single PACE L3M Rrs DAY granule:\n",
    "      - download Rrs via earthaccess\n",
    "      - run BRT CHLA(z) prediction\n",
    "      - build derived metrics\n",
    "      - write daily NetCDF to local temp\n",
    "      - upload NetCDF to GCS\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        GCS path of the uploaded NetCDF (for logging/debugging).\n",
    "    \"\"\"\n",
    "    import earthaccess\n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from google.cloud import storage\n",
    "    import tempfile\n",
    "\n",
    "    # 1. auth for earthaccess on the worker\n",
    "    auth = earthaccess.login(persist=True)\n",
    "\n",
    "    # 2. get date for this granule\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)          # Timestamp\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "\n",
    "    # 3. open Rrs dataset for this granule\n",
    "    files = earthaccess.open([res], auth=auth, pqdm_kwargs={\"disable\": True})\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "\n",
    "    try:\n",
    "        # Rrs for that day\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        # 4. CHLA(z) prediction for this day\n",
    "        # NOTE: bundle and build_chla_profile_dataset must be importable/picklable\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),   # time coord length 1\n",
    "            z_name=\"z\",\n",
    "            silent=True,\n",
    "        )  # (time=1, z, lat, lon), float32\n",
    "\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        # 5. add / update metadata\n",
    "        ds_day[\"CHLA\"].attrs.update(\n",
    "            units=\"mg m-3\",\n",
    "            long_name=\"Chlorophyll-a concentration\",\n",
    "            description=\"BRT-derived CHLA profiles from PACE hyperspectral Rrs\",\n",
    "        )\n",
    "        ds_day[\"z\"].attrs.update(units=\"m\", long_name=\"depth (bin center)\")\n",
    "        ds_day[\"lat\"].attrs.update(units=\"degrees_north\")\n",
    "        ds_day[\"lon\"].attrs.update(units=\"degrees_east\")\n",
    "        ds_day.attrs[\"source\"] = \"BRT model trained on BGC-Argo + OOI matchups\"\n",
    "        ds_day.attrs[\"model_bundle\"] = Path(\"path/to/bundle.zip\").name  # adjust\n",
    "\n",
    "        # 6. write to local temp NetCDF\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "\n",
    "        encoding = {\n",
    "            \"CHLA\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"zlib\": True,\n",
    "                \"complevel\": 4,\n",
    "                \"chunksizes\": (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        ds_day.to_netcdf(\n",
    "            local_path,\n",
    "            engine=\"h5netcdf\",\n",
    "            encoding=encoding,\n",
    "        )\n",
    "\n",
    "        # 7. upload to GCS\n",
    "        storage_client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        blob_path = f\"{destination_prefix}/chla_z_{day_str}.nc\"\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "\n",
    "        # clean up local file\n",
    "        local_path.unlink(missing_ok=True)\n",
    "\n",
    "        gcs_url = f\"gs://{bucket_name}/{blob_path}\"\n",
    "        print(f\"[{day_str}] Uploaded â†’ {gcs_url}\")\n",
    "        return gcs_url\n",
    "\n",
    "    finally:\n",
    "        rrs_ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7487b5f4-8e86-4a77-9de7-f8385a123e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded ML bundle from: models/brt_chla_profiles_bundle.zip\n",
      "  model_kind : pickle\n",
      "  model_type : collection (dict), n_submodels=20\n",
      "  example key: CHLA_0_10\n",
      "  target     : log10_CHLA_A_B depth bins\n",
      "  features   : 174 columns\n",
      "  train/test : 4408 / 1102 rows\n",
      "  dataset    : 5510 rows stored in bundle\n",
      "\n",
      "Usage example (Python):\n",
      "  bundle = load_ml_bundle('path/to/bundle.zip')\n",
      "  # Predict using helper 'predict_all_depths_for_day'\n",
      "  # Example: predict all depths for one day from a BRF dataset R\n",
      "  pred = bundle.predict(\n",
      "      R_dataset,                  # xr.DataArray/xr.Dataset with lat/lon + predictors\n",
      "      brt_models=bundle.model,    # dict of models by depth bin\n",
      "      feature_cols=bundle.meta['feature_cols'],\n",
      "      consts={'solar_hour': 12.0, 'type': 1},\n",
      "  )  # -> e.g. CHLA(time?, z, lat, lon)\n",
      "\n",
      "  # Plot using helper 'make_plot_pred_map'\n",
      "  fig, ax = bundle.plot(pred_da, pred_label='Prediction')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in model\n",
    "\n",
    "## Load model\n",
    "bundle = mu.load_ml_bundle(\"models/brt_chla_profiles_bundle.zip\")\n",
    "brt_models = bundle.model\n",
    "meta = bundle.meta\n",
    "rrs_cols = meta[\"rrs_cols\"]\n",
    "chl_cols = meta[\"y_col\"]\n",
    "extra = meta[\"extra_cols\"]\n",
    "dataset = bundle.data[\"dataset\"]\n",
    "train_idx = bundle.data[\"train_idx\"]\n",
    "test_idx = bundle.data[\"test_idx\"]\n",
    "X_train = bundle.data[\"X_train\"]\n",
    "X_test = bundle.data[\"X_test\"]\n",
    "y_train_all = bundle.data[\"y_train\"]\n",
    "y_test_all = bundle.data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ec293-cd7a-4e9e-8468-ef5e08318e90",
   "metadata": {},
   "source": [
    "# Set up Dask Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6997c-bd0b-489b-a9dc-56778ede8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "\n",
    "# I don't know how to decide. I know that one day takes 5Gb RAM\n",
    "# options.worker_cores = 4\n",
    "# options.worker_memory = \"32GiB\"\n",
    "\n",
    "cluster = gateway.new_cluster(options)\n",
    "\n",
    "# I don't know how to decide\n",
    "# cluster.scale(8)  # say 8 workers\n",
    "\n",
    "# I have 560 days and each day is 30min. I don't want this to take all day\n",
    "cluster.adapt(minimum=4, maximum=16)\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = cluster.get_client()\n",
    "print(cluster)\n",
    "print(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88037da5-0885-4406-8756-508c70f84027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have rrs_results from earthaccess.search_data\n",
    "# maybe subset by date or just take all DAY granules\n",
    "granules = rrs_results[:10]   # or rrs_results[:100] for testing\n",
    "\n",
    "# one Dask task per granule\n",
    "futures = client.map(process_one_granule, granules)\n",
    "\n",
    "# block until all are done, get the GCS URLs (or errors)\n",
    "results = client.gather(futures)\n",
    "print(\"Uploaded daily files:\")\n",
    "for r in results:\n",
    "    print(\"  \", r)\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
