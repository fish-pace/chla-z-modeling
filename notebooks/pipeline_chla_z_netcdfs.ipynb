{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347fbf4d-45e9-4507-8876-a5f1dc4b20ad",
   "metadata": {},
   "source": [
    "# Parallel daily CHLA(z) netcdf production using Dask-Gateway\n",
    "\n",
    "This shows my pipeline for processing netcdfs and posting to a Google Cloud Bucket.\n",
    "\n",
    "[Video of me walking through this notebook](https://youtu.be/mCfMGyKEJgU)\n",
    "\n",
    "1. Get links to Rrs daily L3 files\n",
    "2. Create a function to create predicted CHLA(z) from BRT model\n",
    "3. Create function to start the Gateway cluster\n",
    "4. Run!\n",
    "\n",
    "**A couple gotchas to avoid**\n",
    "\n",
    "* This saves a temp netcdf to /tmp. There is not much room on that (like 10Gb?). If you forget to delete files as you work, then you will max it out and crash your server. If you see lots of server restarts, then that is probably what happened.\n",
    "* Getting authentication to work in Dask Gateway workers can be non-intuitive. Try testing your workflow in a VM where you do not have your credentials stored.\n",
    "* I tried writing cloud bucket to cloud bucket without the temp local write, but that was desperately slow for some reason.\n",
    "* Dask workers do not have access to your home directory, so you'll need to upload any files that they need.\n",
    "* The `process_one_granule()` function does not use a dask graph (no dask-backed arrays processing) but if it did it would be essential that we put those tasks inside `with dask.config.set(scheduler=\"threads\"):`  If we do not, dask will try to spread the dask graph all the workers and you will get cryptic serialization errors.\n",
    "* Be careful not to commit any secrets or passwords to a GitHub repository. Notice how I read those in from local files that are not in the repository.\n",
    "* Your Dask Workers will be happiest if the image for the workers is the same as your notebook. My image was missing google-cloud-storage, so I was trying to pip install that into my workers. That was an enormous hassle and it was easier to create an Docker image with the packages I needed. [Docker image](https://github.com/nmfs-opensci/container-images/tree/main/images/openscapes). With GitHub, it is easy to create and host images that you can use with Dask Gateway (or coiled).\n",
    "\n",
    "### Authentication to cloud buckets\n",
    "\n",
    "Because we are writing to a cloud bucket, we need to deal with authentication and that can cause a lot of headaches when sending tasks to Dask Workers. See the notes at the bottom on how to get the authentication to work. These functions use the google-cloud-storage package to handle authentication.\n",
    "\n",
    "### What is Dask Gateway\n",
    "\n",
    "Dask Gateway is a service that lets users create and manage Dask clusters (virtual machines) on shared infrastructure (often Kubernetes) from a notebook or script, without needing direct access to the underlying compute platform. It must be installed and configured by your JupyterHub / platform administrator (resource limits, images, auth, scaling policies); users can only request clusters once that backend setup is in place.\n",
    "\n",
    "https://coiled.io/ is a similar service and also allows you to run your tasks on whatever cloud provider that you need to. Anyone can use coiled; your admin doesn't need to set up for you.\n",
    "\n",
    "## Set up the `process_one_granule()` function\n",
    "\n",
    "It also has a variety of helper files.\n",
    "\n",
    "- Searches PACE L3M Rrs DAY granules via earthaccess\n",
    "- For each granule/day:\n",
    "    * downloads Rrs\n",
    "    * runs BRT CHLA(z) prediction\n",
    "    * computes integrated/peak metrics\n",
    "    * writes a daily NetCDF locally and deletes when done\n",
    "    * uploads to GCS\n",
    "- Skips days that already exist in GCS unless FORCE_RERUN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585c525e-4fd4-4d4b-af9a-a263a7264298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parallel daily CHLA(z) production using Dask-Gateway.\n",
    "\n",
    "- Searches PACE L3M Rrs DAY granules via earthaccess\n",
    "- For each granule/day:\n",
    "    * downloads Rrs\n",
    "    * runs BRT CHLA(z) prediction\n",
    "    * computes integrated/peak metrics\n",
    "    * writes a daily NetCDF locally\n",
    "    * uploads to GCS\n",
    "- Skips days that already exist in GCS unless FORCE_RERUN=True\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from google.cloud import storage\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Path to your saved ML bundle (zip) – adjust as needed\n",
    "BUNDLE_PATH = \"models/brt_chla_profiles_bundle.zip\"\n",
    "BUNDLE_FILENAME = Path(BUNDLE_PATH).name  # \"brt_chla_profiles_bundle.zip\"\n",
    "\n",
    "# GCS target\n",
    "BUCKET_NAME = \"nmfs_odp_nwfsc\"\n",
    "DESTINATION_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "# Dask-Gateway settings\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_CORES = 4\n",
    "WORKER_MEMORY = \"32GiB\"\n",
    "\n",
    "# Spatial chunking for NetCDF output\n",
    "LAT_CHUNK = 100\n",
    "LON_CHUNK = 100\n",
    "\n",
    "# Rerun control: if False, skip days that already exist in GCS\n",
    "FORCE_RERUN = False\n",
    "\n",
    "# Optional date filtering for rrs_results (None = no filter)\n",
    "START_DATE = None  # e.g. \"2024-03-01\"\n",
    "END_DATE   = None  # e.g. \"2024-04-30\"\n",
    "\n",
    "#START_DATE = \"2024-04-01\"\n",
    "#END_DATE   = \"2024-04-02\"\n",
    "\n",
    "import netrc\n",
    "import json\n",
    "\n",
    "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
    "auth = netrc.netrc(netrc_path)\n",
    "login, account, password = auth.authenticators(\"urs.earthdata.nasa.gov\")\n",
    "ED_USER = login\n",
    "ED_PASS = password\n",
    "with open(\"/home/jovyan/.config/gcloud/application_default_credentials.json\") as f:\n",
    "    GCP_SA_JSON = f.read()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helper: load ML bundle and build CHLA profile dataset\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Ensure ml_utils is available\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    import subprocess\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"-q\",\n",
    "            \"https://raw.githubusercontent.com/fish-pace/chla-z-modeling/main/ml_utils.py\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "import ml_utils as mu  \n",
    "\n",
    "#######################\n",
    "# - Helper; Not needed as I use a Docker image with the packages I need\n",
    "#######################\n",
    "def ensure_google_cloud_storage():\n",
    "    \"\"\"Install google-cloud-storage on the worker if it's missing.\"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "\n",
    "\n",
    "def build_chla_profile_dataset(CHLA: xr.DataArray) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Given CHLA(time, z, lat, lon), compute derived metrics and\n",
    "    return an xr.Dataset suitable for writing to Zarr/NetCDF.\n",
    "    \"\"\"\n",
    "    # Start from CHLA's own dataset so its coords (including z_start/z_end) win\n",
    "    ds = CHLA.to_dataset(name=\"CHLA\")\n",
    "\n",
    "    # ---- Layer thickness (z dimension) ----\n",
    "    z_start = CHLA.coords.get(\"z_start\", None)\n",
    "    z_end   = CHLA.coords.get(\"z_end\", None)\n",
    "\n",
    "    if (z_start is not None) and (z_end is not None):\n",
    "        z_thick = (z_end - z_start).rename(\"z_thickness\")   # (z)\n",
    "    else:\n",
    "        # fallback: uniform layer thickness, e.g. 10 m\n",
    "        z_thick = xr.full_like(CHLA[\"z\"], 10.0).rename(\"z_thickness\")\n",
    "\n",
    "    z_center = CHLA[\"z\"]\n",
    "\n",
    "    # total CHLA in column (used for validity + center-of-mass)\n",
    "    col_total = CHLA.sum(\"z\")          # (time, lat, lon)\n",
    "    valid = col_total > 0              # True where there is some CHLA\n",
    "\n",
    "    # ---- Integrated CHLA (nominal 0–200 m; actual range = z extent) ----\n",
    "    CHLA_int = (CHLA * z_thick).sum(\"z\")\n",
    "    CHLA_int = CHLA_int.where(valid)\n",
    "    CHLA_int.name = \"CHLA_int_0_200\"\n",
    "\n",
    "    # ---- Peak value and depth (NaN-safe) ----\n",
    "    CHLA_filled = CHLA.fillna(-np.inf)\n",
    "    peak_idx = CHLA_filled.argmax(\"z\")       # (time, lat, lon) integer indices\n",
    "\n",
    "    CHLA_peak = CHLA.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak.name = \"CHLA_peak\"\n",
    "\n",
    "    CHLA_peak_depth = z_center.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak_depth.name = \"CHLA_peak_depth\"\n",
    "\n",
    "    # ---- Depth-weighted mean depth (center of mass) ----\n",
    "    num = (CHLA * z_center).sum(\"z\")\n",
    "    den = col_total\n",
    "    depth_cm = (num / den).where(valid)\n",
    "    depth_cm.name = \"CHLA_depth_center_of_mass\"\n",
    "\n",
    "    # ---- Attach derived fields to the dataset ----\n",
    "    ds[\"CHLA_int_0_200\"] = CHLA_int\n",
    "    ds[\"CHLA_peak\"] = CHLA_peak\n",
    "    ds[\"CHLA_peak_depth\"] = CHLA_peak_depth\n",
    "    ds[\"CHLA_depth_center_of_mass\"] = depth_cm\n",
    "    ds[\"z_thickness\"] = z_thick\n",
    "\n",
    "    # ---- Variable attributes ----\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"units\", \"mg m-3\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"long_name\", \"Chlorophyll-a concentration\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"standard_name\", \"mass_concentration_of_chlorophyll_a_in_sea_water\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\n",
    "        \"description\",\n",
    "        \"BRT-derived chlorophyll-a profiles from PACE hyperspectral Rrs\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_int_0_200\"].attrs.update(\n",
    "        units=\"mg m-2\",\n",
    "        long_name=\"Depth-integrated chlorophyll-a\",\n",
    "        description=(\n",
    "            \"Vertical integral of CHLA over the available depth bins \"\n",
    "            \"(nominally 0–200 m; actual range defined by z_start/z_end).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak\"].attrs.update(\n",
    "        units=\"mg m-3\",\n",
    "        long_name=\"Peak chlorophyll-a concentration in the water column\",\n",
    "        standard_name=\"mass_concentration_of_chlorophyll_a_in_sea_water\",\n",
    "        description=\"Maximum CHLA value over depth at each (time, lat, lon).\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak_depth\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Depth of peak chlorophyll-a\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth (bin center) where CHLA is maximal in the water column \"\n",
    "            \"at each (time, lat, lon).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_depth_center_of_mass\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Chlorophyll-a depth center of mass\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth of the chlorophyll-a center of mass, computed as \"\n",
    "            \"sum_z(CHLA * z) / sum_z(CHLA).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"z_thickness\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Layer thickness\",\n",
    "        description=(\n",
    "            \"Thickness of each vertical bin used for depth integration. \"\n",
    "            \"Derived from z_end - z_start when available; otherwise set to a \"\n",
    "            \"uniform nominal thickness.\"\n",
    "        ),\n",
    "    )\n",
    "    ds[\"z_thickness\"] = ds[\"z_thickness\"].expand_dims(time=ds[\"time\"])\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Worker-side function: process ONE granule/day\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def process_one_granule(\n",
    "    res,\n",
    "    lat_chunk=LAT_CHUNK,\n",
    "    lon_chunk=LON_CHUNK,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_prefix=DESTINATION_PREFIX,\n",
    "    force_rerun=FORCE_RERUN,\n",
    "    ed_username=ED_USER,\n",
    "    ed_password=ED_PASS,\n",
    "    gcp_sa_json=GCP_SA_JSON,\n",
    "    bundle_filename=BUNDLE_FILENAME,\n",
    "):\n",
    "    import os\n",
    "    import tempfile\n",
    "    import earthaccess\n",
    "    import xarray as xr\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import ml_utils as mu  # <- now workers can import this\n",
    "\n",
    "    # --- ensure google-cloud-storage is available on THIS worker ---\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "\n",
    "    from google.cloud import storage  # now this should succeed\n",
    "\n",
    "    # --- locate the bundle file next to ml_utils.py ---\n",
    "    bundle_path = Path(mu.__file__).with_name(bundle_filename)\n",
    "    # just to be extra defensive:\n",
    "    if not bundle_path.exists():\n",
    "        raise FileNotFoundError(f\"Bundle not found at {bundle_path}\")\n",
    "\n",
    "    bundle = mu.load_ml_bundle(str(bundle_path))\n",
    "    \n",
    "    # DEBUG Load bundle on the worker from the uploaded zip file\n",
    "    #bundle = mu.load_ml_bundle(bundle_filename)\n",
    "\n",
    "    # --- EARTHACCESS AUTH VIA ENV VARS (inside worker) ---\n",
    "    if ed_username is not None and ed_password is not None:\n",
    "        os.environ[\"EARTHDATA_USERNAME\"] = ed_username\n",
    "        os.environ[\"EARTHDATA_PASSWORD\"] = ed_password\n",
    "\n",
    "    auth = earthaccess.login(strategy=\"environment\", persist=False)\n",
    "\n",
    "    # --- GCP AUTH VIA JSON TEXT (inside worker) ---\n",
    "    import uuid\n",
    "\n",
    "    cred_path = None\n",
    "    if gcp_sa_json:\n",
    "        cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\")\n",
    "        with open(cred_path, \"w\") as f:\n",
    "            f.write(gcp_sa_json)\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "\n",
    "    # -------------------------------\n",
    "    #  Normal per-day pipeline below\n",
    "    # -------------------------------\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "\n",
    "    storage_client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob_path = f\"{destination_prefix}/chla_z_{day_str}_v2.nc\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    if blob.exists() and not force_rerun:\n",
    "        msg = f\"[{day_str}] SKIP (exists at gs://{bucket_name}/{blob_path})\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    files = earthaccess.open([res], pqdm_kwargs={\"disable\": True})\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "    # DEBUG\n",
    "    # rrs_ds = rrs_ds.sel(lat=slice(40, 20), lon=slice(-70, -60) )\n",
    "\n",
    "    try:\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),\n",
    "            z_name=\"z\",\n",
    "            silent=True,\n",
    "            linear=True,\n",
    "        )\n",
    "\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "\n",
    "        # Fix chunking\n",
    "        chunks4d = (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk)\n",
    "        chunks3d = (1, lat_chunk, lon_chunk)  \n",
    "        chunks2d = (1, ds_day.sizes[\"z\"])\n",
    "        encoding = {\n",
    "            \"CHLA\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks4d},\n",
    "            \"CHLA_int_0_200\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks3d},\n",
    "            \"CHLA_peak\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks3d},\n",
    "            \"CHLA_peak_depth\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks3d},\n",
    "            \"CHLA_depth_center_of_mass\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks3d},\n",
    "            \"z_thickness\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks2d},\n",
    "        }\n",
    "\n",
    "        ds_day.to_netcdf(local_path, engine=\"h5netcdf\", encoding=encoding)\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "        # this is crucial. Remove file from /tmp when done. Otherwise /tmp will fill and server will crash.\n",
    "        local_path.unlink(missing_ok=True)\n",
    "\n",
    "        gcs_url = f\"gs://{bucket_name}/{blob_path}\"\n",
    "        msg = f\"[{day_str}] WROTE {gcs_url}\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    finally:\n",
    "        rrs_ds.close()\n",
    "        # optional: clean up the creds file\n",
    "        if cred_path is not None:\n",
    "            try:\n",
    "                os.remove(cred_path)\n",
    "            except FileNotFoundError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73600e1a-37d3-40f8-a2f8-902252a7f279",
   "metadata": {},
   "source": [
    "## Create the function to start Dask Gateway cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362e83b-17cd-4924-b268-71755b2cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# DRIVER: search granules, filter, and dispatch via Dask-Gateway\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # 1. Earthaccess login on client\n",
    "    auth = earthaccess.login(strategy=\"netrc\", persist=True)\n",
    "    if not auth.authenticated:\n",
    "        raise RuntimeError(\"earthaccess login failed\")\n",
    "\n",
    "    # 2. Search PACE L3M Rrs daily granules\n",
    "    rrs_results = earthaccess.search_data(\n",
    "        short_name=\"PACE_OCI_L3M_RRS\",\n",
    "        granule_name=\"*.DAY.*.4km.nc\",\n",
    "        temporal=(START_DATE, END_DATE),\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(rrs_results)} DAY granules after date filter.\")\n",
    "    if not rrs_results:\n",
    "        print(\"Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # 4. Dask-Gateway cluster setup\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    setattr(options, \"worker_resource_allocation\", '4CPU, 30.2Gi')\n",
    "    \n",
    "    cluster = gateway.new_cluster(options)\n",
    "    cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "\n",
    "    client = cluster.get_client()\n",
    "    print(cluster)\n",
    "    print(client)\n",
    "\n",
    "    # Dashboard link (copy/paste into a browser tab)\n",
    "    print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "    # Make sure workers have needed files\n",
    "    client.upload_file(\"ml_utils.py\")\n",
    "    client.upload_file(BUNDLE_PATH)\n",
    "\n",
    "    # ensure google-cloud-storage is installed on every worker\n",
    "    client.run(ensure_google_cloud_storage)\n",
    "\n",
    "    # 5. Dispatch one task per granule\n",
    "    futures = client.map(process_one_granule, rrs_results)\n",
    "\n",
    "    # 6. Stream results as they complete (instead of blocking on gather)\n",
    "    from dask.distributed import as_completed\n",
    "\n",
    "    n = len(futures)\n",
    "    done = 0\n",
    "    errors = 0\n",
    "\n",
    "    try:\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                msg = fut.result()\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] {msg}\")\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] ERROR: {repr(e)}\")\n",
    "                # If you want to stop on first error, uncomment:\n",
    "                # raise\n",
    "    finally:\n",
    "        print(f\"Finished. Success={done - errors}, Errors={errors}\")\n",
    "        client.close()\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b900c-9fe3-4cff-aef4-6aec8a7fa52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 DAY granules after date filter.\n",
      "GatewayCluster<prod.c0c526860a59442a82b1cb067f3c5a4e, status=running>\n",
      "<Client: 'tls://192.168.36.66:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.c0c526860a59442a82b1cb067f3c5a4e/status\n"
     ]
    }
   ],
   "source": [
    "# took 10 hours for 560 files\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144d4abc-3d9f-480d-bf89-8e799c431aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files: 560 first: gcs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240305.nc\n",
      "files: 559 first: gcs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240305_v2.nc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.datetime64('2025-02-06T00:00:00.000000000')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if all the files are there\n",
    "import gcsfs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "token = \"/home/jovyan/.config/gcloud/application_default_credentials.json\"\n",
    "fs = gcsfs.GCSFileSystem(token=token)\n",
    "# all .nc\n",
    "base = \"nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "all_nc = sorted(fs.glob(f\"{base}/chla_z_*.nc\"))\n",
    "# keep only v1 (exclude anything with _v2 anywhere in the name)\n",
    "paths_v1 = [\"gcs://\" + p for p in all_nc if \"_v2\" not in p]\n",
    "paths_v2 = [\"gcs://\" + p for p in all_nc if \"_v2\" in p]\n",
    "print(\"files:\", len(paths_v1), \"first:\", paths_v1[0])\n",
    "print(\"files:\", len(paths_v2), \"first:\", paths_v2[0])\n",
    "\n",
    "day_strs_v1 = [p.split(\"chla_z_\")[1].split(\".nc\")[0] for p in paths_v1]\n",
    "all_times_v1 = np.array(\n",
    "    pd.to_datetime(sorted(set(day_strs_v1)), format=\"%Y%m%d\"),\n",
    "    dtype=\"datetime64[ns]\"\n",
    ")\n",
    "day_strs_v2 = [p.split(\"chla_z_\")[1].split(\"_v2\")[0] for p in paths_v2]\n",
    "all_times_v2 = np.array(\n",
    "    pd.to_datetime(sorted(set(day_strs_v2)), format=\"%Y%m%d\"),\n",
    "    dtype=\"datetime64[ns]\"\n",
    ")\n",
    "missing = [x for x in all_times_v1 if x not in all_times_v2]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50601fdb-1f7c-475b-908a-58ad6f40f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 DAY granules after date filter.\n",
      "GatewayCluster<prod.6cf49325472e4ac4a66bd3af73c1a4bc, status=running>\n",
      "<Client: 'tls://192.168.53.134:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.6cf49325472e4ac4a66bd3af73c1a4bc/status\n"
     ]
    }
   ],
   "source": [
    "# need to rerun a few days\n",
    "START_DATE = '20250206'\n",
    "END_DATE   = '20250206'\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d509d3-62bc-4159-8d8a-4983f334ce31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2f69a-d9da-4185-acd8-76135736bd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
