{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbc84e2-b0db-4efe-8ae2-00d2555a2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /srv/conda/envs/notebook/lib/python3.11/site-packages (3.7.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.43.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.32.5)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (6.33.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.26.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f07560f-cf20-4355-bc97-93696390891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21f8ada-db1e-4335-9cd0-e08c826bad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_google_cloud_storage():\n",
    "    \"\"\"Install google-cloud-storage on the worker if it's missing.\"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "ensure_google_cloud_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc9cb0-d63f-4d98-8e35-d19dba48467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585c525e-4fd4-4d4b-af9a-a263a7264298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parallel daily CHLA(z) production using Dask-Gateway.\n",
    "\n",
    "- Searches PACE L3M Rrs DAY granules via earthaccess\n",
    "- For each granule/day:\n",
    "    * downloads Rrs\n",
    "    * runs BRT CHLA(z) prediction\n",
    "    * computes integrated/peak metrics\n",
    "    * writes a daily NetCDF locally\n",
    "    * uploads to GCS\n",
    "- Skips days that already exist in GCS unless FORCE_RERUN=True\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from google.cloud import storage\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Path to your saved ML bundle (zip) – adjust as needed\n",
    "BUNDLE_PATH = \"models/brt_chla_profiles_bundle.zip\"\n",
    "BUNDLE_FILENAME = Path(BUNDLE_PATH).name  # \"brt_chla_profiles_bundle.zip\"\n",
    "\n",
    "# GCS target\n",
    "BUCKET_NAME = \"nmfs_odp_nwfsc\"\n",
    "DESTINATION_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "# Dask-Gateway settings\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_CORES = 4\n",
    "WORKER_MEMORY = \"32GiB\"\n",
    "\n",
    "# Spatial chunking for NetCDF output\n",
    "LAT_CHUNK = 100\n",
    "LON_CHUNK = 100\n",
    "\n",
    "# Rerun control: if False, skip days that already exist in GCS\n",
    "FORCE_RERUN = False\n",
    "\n",
    "# Optional date filtering for rrs_results (None = no filter)\n",
    "START_DATE = None  # e.g. \"2024-03-01\"\n",
    "END_DATE   = None  # e.g. \"2024-04-30\"\n",
    "\n",
    "#START_DATE = \"2024-04-01\"\n",
    "#END_DATE   = \"2024-04-02\"\n",
    "\n",
    "import netrc\n",
    "import json\n",
    "\n",
    "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
    "auth = netrc.netrc(netrc_path)\n",
    "login, account, password = auth.authenticators(\"urs.earthdata.nasa.gov\")\n",
    "ED_USER = login\n",
    "ED_PASS = password\n",
    "with open(\"/home/jovyan/.config/gcloud/application_default_credentials.json\") as f:\n",
    "    GCP_SA_JSON = f.read()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helper: load ML bundle and build CHLA profile dataset\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Ensure ml_utils is available\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    import subprocess\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"-q\",\n",
    "            \"https://raw.githubusercontent.com/fish-pace/chla-z-modeling/main/ml_utils.py\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "import ml_utils as mu  # noqa: E402\n",
    "\n",
    "# Load the bundle once on the client side; workers will receive it via pickling\n",
    "# DELETE\n",
    "# bundle = mu.load_ml_bundle(BUNDLE_PATH)\n",
    "\n",
    "#######################\n",
    "# - Helper\n",
    "#######################\n",
    "def ensure_google_cloud_storage():\n",
    "    \"\"\"Install google-cloud-storage on the worker if it's missing.\"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "\n",
    "\n",
    "def build_chla_profile_dataset(CHLA: xr.DataArray) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Given CHLA(time, z, lat, lon), compute derived metrics and\n",
    "    return an xr.Dataset suitable for writing to Zarr/NetCDF.\n",
    "    \"\"\"\n",
    "    # Start from CHLA's own dataset so its coords (including z_start/z_end) win\n",
    "    # ADDED\n",
    "    CHLA = xr.apply_ufunc(\n",
    "        np.power,\n",
    "        np.float32(10.0),\n",
    "        CHLA.astype(\"float32\"),\n",
    "        dask=\"allowed\",\n",
    "        output_dtypes=[np.float32],\n",
    "        ).astype(\"float32\")\n",
    "    ds = CHLA.to_dataset(name=\"CHLA\")\n",
    "\n",
    "    # ---- Layer thickness (z dimension) ----\n",
    "    z_start = CHLA.coords.get(\"z_start\", None)\n",
    "    z_end   = CHLA.coords.get(\"z_end\", None)\n",
    "\n",
    "    if (z_start is not None) and (z_end is not None):\n",
    "        z_thick = (z_end - z_start).rename(\"z_thickness\")   # (z)\n",
    "    else:\n",
    "        # fallback: uniform layer thickness, e.g. 10 m\n",
    "        z_thick = xr.full_like(CHLA[\"z\"], 10.0).rename(\"z_thickness\")\n",
    "\n",
    "    z_center = CHLA[\"z\"]\n",
    "\n",
    "    # total CHLA in column (used for validity + center-of-mass)\n",
    "    col_total = CHLA.sum(\"z\")          # (time, lat, lon)\n",
    "    valid = col_total > 0              # True where there is some CHLA\n",
    "\n",
    "    # ---- Integrated CHLA (nominal 0–200 m; actual range = z extent) ----\n",
    "    CHLA_int = (CHLA * z_thick).sum(\"z\")\n",
    "    CHLA_int = CHLA_int.where(valid)\n",
    "    CHLA_int.name = \"CHLA_int_0_200\"\n",
    "\n",
    "    # ---- Peak value and depth (NaN-safe) ----\n",
    "    CHLA_filled = CHLA.fillna(-np.inf)\n",
    "    peak_idx = CHLA_filled.argmax(\"z\")       # (time, lat, lon) integer indices\n",
    "\n",
    "    CHLA_peak = CHLA.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak.name = \"CHLA_peak\"\n",
    "\n",
    "    CHLA_peak_depth = z_center.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak_depth.name = \"CHLA_peak_depth\"\n",
    "\n",
    "    # ---- Depth-weighted mean depth (center of mass) ----\n",
    "    num = (CHLA * z_center).sum(\"z\")\n",
    "    den = col_total\n",
    "    depth_cm = (num / den).where(valid)\n",
    "    depth_cm.name = \"CHLA_depth_center_of_mass\"\n",
    "\n",
    "    # ---- Attach derived fields to the dataset ----\n",
    "    ds[\"CHLA_int_0_200\"] = CHLA_int\n",
    "    ds[\"CHLA_peak\"] = CHLA_peak\n",
    "    ds[\"CHLA_peak_depth\"] = CHLA_peak_depth\n",
    "    ds[\"CHLA_depth_center_of_mass\"] = depth_cm\n",
    "    ds[\"z_thickness\"] = z_thick\n",
    "\n",
    "    # ---- Variable attributes ----\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"units\", \"mg m-3\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"long_name\", \"Chlorophyll-a concentration\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"standard_name\", \"mass_concentration_of_chlorophyll_a_in_sea_water\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\n",
    "        \"description\",\n",
    "        \"BRT-derived chlorophyll-a profiles from PACE hyperspectral Rrs\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_int_0_200\"].attrs.update(\n",
    "        units=\"mg m-2\",\n",
    "        long_name=\"Depth-integrated chlorophyll-a\",\n",
    "        description=(\n",
    "            \"Vertical integral of CHLA over the available depth bins \"\n",
    "            \"(nominally 0–200 m; actual range defined by z_start/z_end).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak\"].attrs.update(\n",
    "        units=\"mg m-3\",\n",
    "        long_name=\"Peak chlorophyll-a concentration in the water column\",\n",
    "        standard_name=\"mass_concentration_of_chlorophyll_a_in_sea_water\",\n",
    "        description=\"Maximum CHLA value over depth at each (time, lat, lon).\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak_depth\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Depth of peak chlorophyll-a\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth (bin center) where CHLA is maximal in the water column \"\n",
    "            \"at each (time, lat, lon).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_depth_center_of_mass\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Chlorophyll-a depth center of mass\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth of the chlorophyll-a center of mass, computed as \"\n",
    "            \"sum_z(CHLA * z) / sum_z(CHLA).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"z_thickness\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Layer thickness\",\n",
    "        description=(\n",
    "            \"Thickness of each vertical bin used for depth integration. \"\n",
    "            \"Derived from z_end - z_start when available; otherwise set to a \"\n",
    "            \"uniform nominal thickness.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Worker-side function: process ONE granule/day\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def process_one_granule(\n",
    "    res,\n",
    "    lat_chunk=LAT_CHUNK,\n",
    "    lon_chunk=LON_CHUNK,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_prefix=DESTINATION_PREFIX,\n",
    "    force_rerun=FORCE_RERUN,\n",
    "    ed_username=ED_USER,\n",
    "    ed_password=ED_PASS,\n",
    "    gcp_sa_json=GCP_SA_JSON,\n",
    "    bundle_filename=BUNDLE_FILENAME,\n",
    "):\n",
    "    import os\n",
    "    import tempfile\n",
    "    import earthaccess\n",
    "    import xarray as xr\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import ml_utils as mu  # <- now workers can import this\n",
    "\n",
    "    # --- ensure google-cloud-storage is available on THIS worker ---\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "\n",
    "    from google.cloud import storage  # now this should succeed\n",
    "\n",
    "    # --- locate the bundle file next to ml_utils.py ---\n",
    "    bundle_path = Path(mu.__file__).with_name(bundle_filename)\n",
    "    # just to be extra defensive:\n",
    "    if not bundle_path.exists():\n",
    "        raise FileNotFoundError(f\"Bundle not found at {bundle_path}\")\n",
    "\n",
    "    bundle = mu.load_ml_bundle(str(bundle_path))\n",
    "    \n",
    "    # DEBUG Load bundle on the worker from the uploaded zip file\n",
    "    #bundle = mu.load_ml_bundle(bundle_filename)\n",
    "\n",
    "    # --- EARTHACCESS AUTH VIA ENV VARS (inside worker) ---\n",
    "    if ed_username is not None and ed_password is not None:\n",
    "        os.environ[\"EARTHDATA_USERNAME\"] = ed_username\n",
    "        os.environ[\"EARTHDATA_PASSWORD\"] = ed_password\n",
    "\n",
    "    auth = earthaccess.login(strategy=\"environment\", persist=False)\n",
    "\n",
    "    # --- GCP AUTH VIA JSON TEXT (inside worker) ---\n",
    "    import uuid\n",
    "\n",
    "    cred_path = None\n",
    "    if gcp_sa_json:\n",
    "        cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\")\n",
    "        with open(cred_path, \"w\") as f:\n",
    "            f.write(gcp_sa_json)\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "\n",
    "    # -------------------------------\n",
    "    #  Normal per-day pipeline below\n",
    "    # -------------------------------\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "\n",
    "    storage_client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob_path = f\"{destination_prefix}/chla_z_{day_str}_v2.nc\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    if blob.exists() and not force_rerun:\n",
    "        msg = f\"[{day_str}] SKIP (exists at gs://{bucket_name}/{blob_path})\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    files = earthaccess.open([res], pqdm_kwargs={\"disable\": True})\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "    # debug\n",
    "    # rrs_ds = rrs_ds.sel(lat=slice(40, 20), lon=slice(-70, -60) )\n",
    "\n",
    "    try:\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),\n",
    "            z_name=\"z\",\n",
    "            silent=True,\n",
    "        )\n",
    "\n",
    "        # ADDED\n",
    "        pred = pred.compute()\n",
    "\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "\n",
    "        encoding = {\n",
    "            \"CHLA\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"zlib\": True,\n",
    "                \"complevel\": 4,\n",
    "                \"chunksizes\": (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        ds_day.to_netcdf(local_path, engine=\"h5netcdf\", encoding=encoding)\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "        local_path.unlink(missing_ok=True)\n",
    "\n",
    "        gcs_url = f\"gs://{bucket_name}/{blob_path}\"\n",
    "        msg = f\"[{day_str}] WROTE {gcs_url}\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    finally:\n",
    "        rrs_ds.close()\n",
    "        # optional: clean up the creds file\n",
    "        if cred_path is not None:\n",
    "            try:\n",
    "                os.remove(cred_path)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# DRIVER: search granules, filter, and dispatch via Dask-Gateway\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # 1. Earthaccess login on client\n",
    "    auth = earthaccess.login(strategy=\"netrc\", persist=True)\n",
    "    if not auth.authenticated:\n",
    "        raise RuntimeError(\"earthaccess login failed\")\n",
    "\n",
    "    # 2. Search PACE L3M Rrs daily granules\n",
    "    rrs_results = earthaccess.search_data(\n",
    "        short_name=\"PACE_OCI_L3M_RRS\",\n",
    "        granule_name=\"*.DAY.*.4km.nc\",\n",
    "        temporal=(START_DATE, END_DATE),\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(rrs_results)} DAY granules after date filter.\")\n",
    "    if not rrs_results:\n",
    "        print(\"Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # 4. Dask-Gateway cluster setup\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    setattr(options, \"worker_resource_allocation\", '4CPU, 30.2Gi')\n",
    "    \n",
    "    cluster = gateway.new_cluster(options)\n",
    "    cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "\n",
    "    client = cluster.get_client()\n",
    "    print(cluster)\n",
    "    print(client)\n",
    "\n",
    "    # Dashboard link (copy/paste into a browser tab)\n",
    "    print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "    # Make sure workers have needed files\n",
    "    client.upload_file(\"ml_utils.py\")\n",
    "    client.upload_file(BUNDLE_PATH)\n",
    "\n",
    "    # ensure google-cloud-storage is installed on every worker\n",
    "    client.run(ensure_google_cloud_storage)\n",
    "\n",
    "    # 5. Dispatch one task per granule\n",
    "    futures = client.map(process_one_granule, rrs_results)\n",
    "\n",
    "    # 6. Stream results as they complete (instead of blocking on gather)\n",
    "    from dask.distributed import as_completed\n",
    "\n",
    "    n = len(futures)\n",
    "    done = 0\n",
    "    errors = 0\n",
    "\n",
    "    try:\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                msg = fut.result()\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] {msg}\")\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] ERROR: {repr(e)}\")\n",
    "                # If you want to stop on first error, uncomment:\n",
    "                # raise\n",
    "    finally:\n",
    "        print(f\"Finished. Success={done - errors}, Errors={errors}\")\n",
    "        client.close()\n",
    "        cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b900c-9fe3-4cff-aef4-6aec8a7fa52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 DAY granules after date filter.\n",
      "GatewayCluster<prod.b67ca36cb9b748ed97489bb7f0751806, status=running>\n",
      "<Client: 'tls://192.168.55.182:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.b67ca36cb9b748ed97489bb7f0751806/status\n"
     ]
    }
   ],
   "source": [
    "# took 10 hours for 560 files\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d4abc-3d9f-480d-bf89-8e799c431aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11b735-b374-472c-9a5f-cb2f048fa84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd12074-148e-4d94-8a01-ab96cc9dd8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50601fdb-1f7c-475b-908a-58ad6f40f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 DAY granules after date filter.\n",
      "GatewayCluster<prod.0d2bdc79e25a43f3b9a184b8bb2c7020, status=running>\n",
      "<Client: 'tls://192.168.35.98:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.0d2bdc79e25a43f3b9a184b8bb2c7020/status\n",
      "[1/3] [20250211] WROTE gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20250211.nc\n",
      "[2/3] [20250210] WROTE gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20250210.nc\n",
      "[3/3] [20250209] WROTE gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20250209.nc\n",
      "Finished. Success=3, Errors=0\n"
     ]
    }
   ],
   "source": [
    "# need to rerun a few days\n",
    "START_DATE = '20250209'\n",
    "END_DATE   = '20250211'\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89769d4-9f8d-439d-a575-87d930f2d0d6",
   "metadata": {},
   "source": [
    "# Process Zarr\n",
    "\n",
    "Should have baked this into the first pipeline. Alas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b19974d-2fdd-49e1-ae3e-6a3949f863a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chla_zarr_worker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chla_zarr_worker.py\n",
    "import os, uuid, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import zarr\n",
    "\n",
    "def normalize_time(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if \"time\" in ds.coords:\n",
    "        ds = ds.assign_coords(time=ds[\"time\"].astype(\"datetime64[ns]\"))\n",
    "    return ds\n",
    "\n",
    "def make_time_only_for_region(ds: xr.Dataset) -> xr.Dataset:\n",
    "    time_vars = [v for v, da in ds.data_vars.items() if \"time\" in da.dims]\n",
    "    if not time_vars:\n",
    "        raise ValueError(\"No time-dependent variables found for region write.\")\n",
    "    ds_time = ds[time_vars]\n",
    "    drop_coord_vars = [c for c in list(ds_time.coords) if c != \"time\"]\n",
    "    if drop_coord_vars:\n",
    "        ds_time = ds_time.drop_vars(drop_coord_vars)\n",
    "    ds_time = ds_time.assign_coords(time=ds_time[\"time\"].astype(\"datetime64[ns]\"))\n",
    "    return ds_time\n",
    "\n",
    "def chunk_spec(ds: xr.Dataset, lat_chunk: int, lon_chunk: int) -> dict:\n",
    "    spec = {}\n",
    "    if \"time\" in ds.dims: spec[\"time\"] = 1\n",
    "    if \"z\" in ds.dims:    spec[\"z\"] = ds.sizes[\"z\"]\n",
    "    if \"lat\" in ds.dims:  spec[\"lat\"] = lat_chunk\n",
    "    if \"lon\" in ds.dims:  spec[\"lon\"] = lon_chunk\n",
    "    return spec\n",
    "\n",
    "def assert_dimension_names_present(mapper, ds: xr.Dataset) -> None:\n",
    "    root = zarr.open_group(mapper, mode=\"r\")\n",
    "    def has_dn(a):\n",
    "        return bool(getattr(a.metadata, \"dimension_names\", None))\n",
    "    if \"time\" not in root or not has_dn(root[\"time\"]):\n",
    "        raise KeyError(\"Store missing v3 dimension_names for 'time' (recreate store at fresh path).\")\n",
    "    for v in ds.data_vars:\n",
    "        if v not in root or not has_dn(root[v]):\n",
    "            raise KeyError(f\"Store missing v3 dimension_names for {v!r} (recreate store at fresh path).\")\n",
    "\n",
    "def write_one_day_region(gcs_url: str, time_index: int, zarr_path: str, gcp_sa_json: str,\n",
    "                         lat_chunk: int, lon_chunk: int) -> str:\n",
    "    # auth on worker\n",
    "    cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\")\n",
    "    with open(cred_path, \"w\") as f:\n",
    "        f.write(gcp_sa_json)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "\n",
    "    try:\n",
    "        fs = gcsfs.GCSFileSystem(token=\"google_default\")\n",
    "        mapper = fs.get_mapper(zarr_path)\n",
    "\n",
    "        tmp_dir = Path(\"/tmp/chla_nc_to_zarr_workers\")\n",
    "        tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        local_nc = tmp_dir / Path(gcs_url).name\n",
    "\n",
    "        fs.get(gcs_url, str(local_nc))\n",
    "\n",
    "        ds = xr.open_dataset(local_nc, engine=\"h5netcdf\")\n",
    "        try:\n",
    "            ds = normalize_time(ds)\n",
    "            ds_time = make_time_only_for_region(ds).chunk(chunk_spec(ds, lat_chunk, lon_chunk))\n",
    "\n",
    "            assert_dimension_names_present(mapper, ds_time)\n",
    "\n",
    "            region = {\"time\": slice(time_index, time_index + 1)}\n",
    "            ds_time.to_zarr(mapper, mode=\"r+\", region=region, consolidated=False, zarr_version=3)\n",
    "        finally:\n",
    "            ds.close()\n",
    "            local_nc.unlink(missing_ok=True)\n",
    "\n",
    "        return f\"OK {Path(gcs_url).name} -> time_index={time_index}\"\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1bc9f4b-d4f1-4268-9e9f-2ed367c45ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Gateway.__del__ at 0x7f5af1c756c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/dask_gateway/client.py\", line 380, in __del__\n",
      "    self.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/dask_gateway/client.py\", line 353, in close\n",
      "    elif self.loop.asyncio_loop.is_running():\n",
      "         ^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/dask_gateway/client.py\", line 330, in loop\n",
      "    return self._loop_runner.loop\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/distributed/utils.py\", line 648, in loop\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Accessing the loop property while the loop is not running is not supported\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import zarr\n",
    "\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "import chla_zarr_worker as wz  # <-- your uploaded module\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG (edit as needed)\n",
    "# =============================================================================\n",
    "TOKEN_PATH = \"/home/jovyan/.config/gcloud/application_default_credentials.json\"\n",
    "\n",
    "BUCKET = \"nmfs_odp_nwfsc\"\n",
    "NETCDF_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "NETCDF_PATTERN = f\"{BUCKET}/{NETCDF_PREFIX}/chla_z_*.nc\"\n",
    "\n",
    "# IMPORTANT: start with a fresh v3 path\n",
    "ZARR_PATH = f\"gcs://{BUCKET}/CB/fish-pace-datasets/chla-z/zarr_v12\"\n",
    "\n",
    "LAT_CHUNK = 128\n",
    "LON_CHUNK = 128\n",
    "\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_RESOURCE = \"4CPU, 30.2Gi\"\n",
    "\n",
    "# =============================================================================\n",
    "# LISTING + TIME AXIS\n",
    "# =============================================================================\n",
    "_date_re = re.compile(r\"chla_z_(\\d{8})\\.nc$\")\n",
    "\n",
    "\n",
    "def date_from_url(gcs_url: str) -> pd.Timestamp:\n",
    "    m = _date_re.search(gcs_url)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse date from: {gcs_url}\")\n",
    "    return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "\n",
    "def list_netcdf_urls(fs: gcsfs.GCSFileSystem) -> list[str]:\n",
    "    paths = sorted(fs.glob(NETCDF_PATTERN))\n",
    "    urls = [\"gcs://\" + p for p in paths]\n",
    "    return sorted(urls, key=date_from_url)\n",
    "\n",
    "\n",
    "def build_time_index(urls: list[str]) -> pd.DatetimeIndex:\n",
    "    times = pd.to_datetime([date_from_url(u) for u in urls]).astype(\"datetime64[ns]\")\n",
    "    return pd.DatetimeIndex(times, name=\"time\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AUTH HELPERS\n",
    "# =============================================================================\n",
    "def read_sa_json(token_path: str = TOKEN_PATH) -> str:\n",
    "    with open(token_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def make_gcsfs_with_sa_json(gcp_sa_json: str) -> tuple[gcsfs.GCSFileSystem, str]:\n",
    "    cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_{uuid.uuid4().hex}.json\")\n",
    "    with open(cred_path, \"w\") as f:\n",
    "        f.write(gcp_sa_json)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "    fs = gcsfs.GCSFileSystem(token=\"google_default\")\n",
    "    return fs, cred_path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ZARR v3 TEMPLATE (dimension_names required)\n",
    "# =============================================================================\n",
    "def ensure_store_is_empty(fs: gcsfs.GCSFileSystem, zarr_path: str) -> None:\n",
    "    if not zarr_path.startswith(\"gcs://\"):\n",
    "        raise ValueError(\"Expected zarr_path like gcs://bucket/prefix\")\n",
    "    no_scheme = zarr_path[len(\"gcs://\") :]\n",
    "    try:\n",
    "        existing = fs.ls(no_scheme)\n",
    "    except FileNotFoundError:\n",
    "        existing = []\n",
    "    if existing:\n",
    "        raise RuntimeError(\n",
    "            f\"Target ZARR_PATH is not empty:\\n  {zarr_path}\\n\"\n",
    "            \"Pick a NEW ZARR_PATH or delete the existing prefix before running.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def normalize_time(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if \"time\" in ds.coords:\n",
    "        ds = ds.assign_coords(time=ds[\"time\"].astype(\"datetime64[ns]\"))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_zarr_v3_template_direct(\n",
    "    gcp_sa_json: str,\n",
    "    sample_url: str,\n",
    "    times: pd.DatetimeIndex,\n",
    "    zarr_path: str,\n",
    "    lat_chunk: int,\n",
    "    lon_chunk: int,\n",
    ") -> None:\n",
    "    fs, cred_path = make_gcsfs_with_sa_json(gcp_sa_json)\n",
    "    try:\n",
    "        ensure_store_is_empty(fs, zarr_path)\n",
    "\n",
    "        tmp_dir = Path(\"/tmp/chla_zarr_template_v3\")\n",
    "        tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        local_nc = tmp_dir / Path(sample_url).name\n",
    "        fs.get(sample_url, str(local_nc))\n",
    "\n",
    "        ds0 = xr.open_dataset(local_nc, engine=\"h5netcdf\")\n",
    "        try:\n",
    "            ds0 = normalize_time(ds0)\n",
    "            nt = len(times)\n",
    "\n",
    "            mapper = fs.get_mapper(zarr_path)\n",
    "            root = zarr.group(store=mapper, overwrite=True, zarr_format=3)\n",
    "\n",
    "            # time coordinate (int64 ns)\n",
    "            time_int = times.values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "            root.create_array(\n",
    "                name=\"time\",\n",
    "                shape=(nt,),\n",
    "                chunks=(min(nt, 1024),),\n",
    "                dtype=\"int64\",\n",
    "                dimension_names=(\"time\",),\n",
    "                overwrite=True,\n",
    "            )\n",
    "            root[\"time\"][:] = time_int\n",
    "\n",
    "            # coords: use data=..., and DO NOT pass dtype/shape with data\n",
    "            for cname in [\"lat\", \"lon\", \"z\", \"z_start\", \"z_end\"]:\n",
    "                if cname in ds0.coords:\n",
    "                    vals = np.asarray(ds0[cname].values)\n",
    "                    dims = tuple(ds0[cname].dims)\n",
    "                    root.create_array(\n",
    "                        name=cname,\n",
    "                        data=vals,\n",
    "                        chunks=vals.shape,\n",
    "                        dimension_names=dims,\n",
    "                        overwrite=True,\n",
    "                    )\n",
    "\n",
    "            # data vars: create empty arrays for region writes\n",
    "            for v in ds0.data_vars:\n",
    "                da0 = ds0[v]\n",
    "                dims = tuple(da0.dims)\n",
    "\n",
    "                shape: list[int] = []\n",
    "                chunks: list[int] = []\n",
    "                for d in dims:\n",
    "                    if d == \"time\":\n",
    "                        shape.append(nt); chunks.append(1)\n",
    "                    elif d == \"z\":\n",
    "                        shape.append(ds0.sizes[\"z\"]); chunks.append(ds0.sizes[\"z\"])\n",
    "                    elif d == \"lat\":\n",
    "                        shape.append(ds0.sizes[\"lat\"]); chunks.append(lat_chunk)\n",
    "                    elif d == \"lon\":\n",
    "                        shape.append(ds0.sizes[\"lon\"]); chunks.append(lon_chunk)\n",
    "                    else:\n",
    "                        shape.append(ds0.sizes[d]); chunks.append(ds0.sizes[d])\n",
    "\n",
    "                fill_value = float(\"nan\") if np.issubdtype(da0.dtype, np.floating) else 0\n",
    "\n",
    "                root.create_array(\n",
    "                    name=v,\n",
    "                    shape=tuple(shape),\n",
    "                    chunks=tuple(chunks),\n",
    "                    dtype=da0.dtype,\n",
    "                    fill_value=fill_value,\n",
    "                    dimension_names=dims,\n",
    "                    overwrite=True,\n",
    "                )\n",
    "\n",
    "        finally:\n",
    "            ds0.close()\n",
    "            local_nc.unlink(missing_ok=True)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "def zarr_main():\n",
    "    gcp_sa_json = read_sa_json(TOKEN_PATH)\n",
    "\n",
    "    # list urls on client\n",
    "    fs, cred_path = make_gcsfs_with_sa_json(gcp_sa_json)\n",
    "    try:\n",
    "        urls = list_netcdf_urls(fs)\n",
    "        if not urls:\n",
    "            raise RuntimeError(\"No NetCDF files found with pattern: \" + NETCDF_PATTERN)\n",
    "        print(\"nfiles:\", len(urls), \"first:\", urls[0])\n",
    "        times = build_time_index(urls)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    print(f\"Creating Zarr v3 template at: {ZARR_PATH}\")\n",
    "    create_zarr_v3_template_direct(\n",
    "        gcp_sa_json=gcp_sa_json,\n",
    "        sample_url=urls[0],\n",
    "        times=times,\n",
    "        zarr_path=ZARR_PATH,\n",
    "        lat_chunk=LAT_CHUNK,\n",
    "        lon_chunk=LON_CHUNK,\n",
    "    )\n",
    "    print(\"Template created:\", ZARR_PATH)\n",
    "\n",
    "    # Dask-Gateway cluster\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    setattr(options, \"worker_resource_allocation\", WORKER_RESOURCE)\n",
    "\n",
    "    cluster = gateway.new_cluster(options)\n",
    "    cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "    client = cluster.get_client()\n",
    "\n",
    "    print(cluster)\n",
    "    print(client)\n",
    "    print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "    # ensure workers have your module\n",
    "    client.upload_file(\"chla_zarr_worker.py\")\n",
    "    client.run(lambda: __import__(\"chla_zarr_worker\").__name__)\n",
    "\n",
    "    # (optional) broadcast JSON once instead of embedding in every task payload\n",
    "    gcp_sa_json_fut = client.scatter(gcp_sa_json, broadcast=True)\n",
    "\n",
    "    futures = []\n",
    "    fut_to_info = {}\n",
    "    for idx, url in enumerate(urls):\n",
    "        fut = client.submit(\n",
    "            wz.write_one_day_region,\n",
    "            url,\n",
    "            idx,\n",
    "            ZARR_PATH,\n",
    "            gcp_sa_json_fut,  # <- scattered once\n",
    "            LAT_CHUNK,\n",
    "            LON_CHUNK,\n",
    "            pure=False,\n",
    "        )\n",
    "        futures.append(fut)\n",
    "        fut_to_info[fut.key] = (idx, url)\n",
    "\n",
    "    n = len(futures)\n",
    "    done = 0\n",
    "    errors = 0\n",
    "\n",
    "    try:\n",
    "        for fut in as_completed(futures):\n",
    "            done += 1\n",
    "            idx, url = fut_to_info.get(fut.key, (None, None))\n",
    "            try:\n",
    "                msg = fut.result()\n",
    "                print(f\"[{done}/{n}] {msg}\")\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                print(f\"[{done}/{n}] ERROR on idx={idx} url={url}\")\n",
    "                print(f\"Exception: {type(e).__name__}: {e}\")\n",
    "\n",
    "        print(f\"Finished region writes. Success={n - errors}, Errors={errors}\")\n",
    "\n",
    "    finally:\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a9bb9-4d3a-4623-93b3-9ec18dfc2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d1c2b-f307-4d14-b331-597bd967302a",
   "metadata": {},
   "source": [
    "## Reprocess the netcdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a958453c-3c9e-457a-9da3-9d081b18e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files: 560 first: gcs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240305.nc\n"
     ]
    }
   ],
   "source": [
    "# Reprocess the netcdfs\n",
    "from pathlib import Path\n",
    "import gcsfs\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from google.cloud import storage\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "token = \"/home/jovyan/.config/gcloud/application_default_credentials.json\"\n",
    "fs = gcsfs.GCSFileSystem(token=token)\n",
    "BUCKET_NAME = \"nmfs_odp_nwfsc\"\n",
    "DESTINATION_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "pattern = f\"{BUCKET_NAME}/{DESTINATION_PREFIX}/chla_z_*.nc\"\n",
    "paths = [\n",
    "    \"gcs://\" + p\n",
    "    for p in sorted(fs.glob(pattern))\n",
    "    if \"_v2\" not in p\n",
    "]\n",
    "print(\"files:\", len(paths), \"first:\", paths[0])\n",
    "\n",
    "# Dask-Gateway settings\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_CORES = 4\n",
    "WORKER_MEMORY = \"32GiB\"\n",
    "\n",
    "# Spatial chunking for NetCDF output\n",
    "LAT_CHUNK = 100\n",
    "LON_CHUNK = 100\n",
    "\n",
    "# Rerun control: if False, skip days that already exist in GCS\n",
    "FORCE_RERUN = False\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"/home/jovyan/.config/gcloud/application_default_credentials.json\") as f:\n",
    "    GCP_SA_JSON = f.read()\n",
    "\n",
    "\n",
    "\n",
    "def reprocess_one_granule(\n",
    "    path,\n",
    "    lat_chunk=LAT_CHUNK,\n",
    "    lon_chunk=LON_CHUNK,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_prefix=DESTINATION_PREFIX,\n",
    "    force_rerun=FORCE_RERUN,\n",
    "    gcp_sa_json=GCP_SA_JSON,\n",
    "):\n",
    "    import os\n",
    "    import tempfile\n",
    "    import earthaccess\n",
    "    import xarray as xr\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import ml_utils as mu  # <- now workers can import this\n",
    "    from pathlib import PurePosixPath\n",
    "    ds = None\n",
    "    ds_day = None\n",
    "    local_nc = None\n",
    "    local_path = None\n",
    "    cred_path = None\n",
    "\n",
    "    # Get the file name\n",
    "    p = PurePosixPath(path)\n",
    "    stem = p.stem\n",
    "\n",
    "    from google.cloud import storage  \n",
    "\n",
    "    # --- GCP AUTH VIA JSON TEXT (inside worker) ---\n",
    "    import uuid\n",
    "\n",
    "    cred_path = None\n",
    "    if gcp_sa_json:\n",
    "        cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\")\n",
    "        with open(cred_path, \"w\") as f:\n",
    "            f.write(gcp_sa_json)\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "\n",
    "    # -------------------------------\n",
    "    #  Normal per-day pipeline below\n",
    "    # -------------------------------\n",
    "\n",
    "    storage_client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob_path = f\"{destination_prefix}/{stem}_v2.nc\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    if blob.exists() and not force_rerun:\n",
    "        msg = f\"[{stem}] SKIP (exists at gs://{bucket_name}/{blob_path})\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    try:\n",
    "        # unique temp dir per task (prevents collisions + unexpected cleanup effects)\n",
    "        with tempfile.TemporaryDirectory(prefix=\"chla_z_\") as td:\n",
    "            tmp_dir = Path(td)\n",
    "            local_nc = tmp_dir / Path(path).name\n",
    "            blob_path_v1 = f\"{destination_prefix}/{stem}.nc\"\n",
    "            #blob_v1 = bucket.blob(blob_path_v1)\n",
    "            #blob_v1.download_to_filename(local_nc)\n",
    "            #if not local_nc.exists():\n",
    "            #    raise FileNotFoundError(f\"Download failed, missing {local_nc}\")\n",
    "\n",
    "            gcs_url = f\"gs://{bucket_name}/{blob_path_v1}\"\n",
    "            ds = xr.open_dataset(gcs_url, engine=\"h5netcdf\", chunks={})\n",
    "                                 \n",
    "            # ensure file handle lifecycle is clean\n",
    "            #ds = xr.open_dataset(local_nc, engine=\"h5netcdf\", chunks=None)\n",
    "            ds[\"z_thickness\"] = ds[\"z_thickness\"].expand_dims(time=ds[\"time\"])\n",
    "            ds = ds.chunk({\"time\": 1, \"z\": -1, \"lat\": lat_chunk, \"lon\": lon_chunk})\n",
    "            #ds.load()  # blocking\n",
    "\n",
    "            ds_day = build_chla_profile_dataset(ds[\"CHLA\"])\n",
    "\n",
    "            local_path = tmp_dir / f\"{stem}_metrics.nc\"\n",
    "            encoding = {\n",
    "                    \"CHLA\": {\n",
    "                        \"dtype\": \"float32\",\n",
    "                        \"zlib\": True,\n",
    "                        \"complevel\": 4,\n",
    "                        \"chunksizes\": (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk),\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            ds_day.to_netcdf(local_path, engine=\"h5netcdf\", encoding=encoding)\n",
    "\n",
    "            blob.upload_from_filename(str(local_path))\n",
    "            return f\"[{stem}] WROTE gs://{bucket_name}/{blob_path}\"\n",
    "\n",
    "    finally:\n",
    "        # close only if created\n",
    "        if ds_day is not None:\n",
    "            ds_day.close()\n",
    "        if ds is not None:\n",
    "            ds.close()\n",
    "\n",
    "        # delete temp files if they exist\n",
    "        if local_path is not None:\n",
    "            local_path.unlink(missing_ok=True)\n",
    "        if local_nc is not None:\n",
    "            local_nc.unlink(missing_ok=True)\n",
    "\n",
    "        # remove creds file if created\n",
    "        if cred_path is not None:\n",
    "            try:\n",
    "                os.remove(cred_path)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "\n",
    "def reprocess_main():\n",
    "\n",
    "    # 4. Dask-Gateway cluster setup\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    setattr(options, \"worker_resource_allocation\", '4CPU, 30.2Gi')\n",
    "#    setattr(options, \"worker_resource_allocation\", '8CPU, 60.4Gi')\n",
    "    \n",
    "    cluster = gateway.new_cluster(options)\n",
    "    cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "\n",
    "    client = cluster.get_client()\n",
    "    print(cluster)\n",
    "    print(client)\n",
    "\n",
    "    # Dashboard link (copy/paste into a browser tab)\n",
    "    print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "    # Make sure workers have needed files\n",
    "    client.upload_file(\"reprocess.py\")\n",
    "    client.run(lambda: __import__(\"reprocess\"))\n",
    "\n",
    "    from reprocess_worker import one_granule\n",
    "    futures = client.map(one_granule, paths)\n",
    "\n",
    "    # 5. Dispatch one task per granule\n",
    "    #futures = client.map(reprocess_one_granule, paths)\n",
    "    futures = client.map(one_granule, paths)\n",
    "\n",
    "    # 6. Stream results as they complete (instead of blocking on gather)\n",
    "    from dask.distributed import as_completed\n",
    "\n",
    "    n = len(futures)\n",
    "    done = 0\n",
    "    errors = 0\n",
    "\n",
    "    try:\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                msg = fut.result()\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] {msg}\")\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] ERROR: {repr(e)}\")\n",
    "                # If you want to stop on first error, uncomment:\n",
    "                # raise\n",
    "    finally:\n",
    "        print(f\"Finished. Success={done - errors}, Errors={errors}\")\n",
    "        client.close()\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03abc7ae-178d-4bcf-8af6-4ff7e47633ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GatewayCluster<prod.e652c7f3ffbc4d559ca6570cbecc63d2, status=running>\n",
      "<Client: 'tls://192.168.43.212:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.e652c7f3ffbc4d559ca6570cbecc63d2/status\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'one_granule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# took 10 hours for 560 files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mreprocess_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 292\u001b[39m, in \u001b[36mreprocess_main\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    288\u001b[39m client.upload_file(\u001b[33m\"\u001b[39m\u001b[33mml_utils.py\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# 5. Dispatch one task per granule\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;66;03m#futures = client.map(reprocess_one_granule, paths)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m futures = client.map(\u001b[43mone_granule\u001b[49m, paths)\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# 6. Stream results as they complete (instead of blocking on gather)\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m as_completed\n",
      "\u001b[31mNameError\u001b[39m: name 'one_granule' is not defined"
     ]
    }
   ],
   "source": [
    "# took 10 hours for 560 files\n",
    "reprocess_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37c2ade-7a20-4eb8-af2b-d0584a990d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 24.8 s, total: 2min 37s\n",
      "Wall time: 3min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[chla_z_20240308] WROTE gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240308_v2.nc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "reprocess_one_granule(paths[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275f8362-830a-4b2d-9e22-ee5d9dc152e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LocalCluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cluster = \u001b[43mLocalCluster\u001b[49m(n_workers=\u001b[32m4\u001b[39m, processes=\u001b[38;5;28;01mFalse\u001b[39;00m, threads_per_worker=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'LocalCluster' is not defined"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=4, processes=False, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d509d3-62bc-4159-8d8a-4983f334ce31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4e82a62-006a-4135-878f-2e2beb462b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 58s, sys: 1min 3s, total: 3min 2s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds_day = one_granule(paths[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58510bcd-cb06-4a72-84db-6bb63b3d0f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d75a06c-7aca-47ec-8607-091a67b80cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `fs'\n",
      "/bin/bash: -c: line 1: `rm (fs)'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2f69a-d9da-4185-acd8-76135736bd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
