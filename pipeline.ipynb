{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbc84e2-b0db-4efe-8ae2-00d2555a2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /srv/conda/envs/notebook/lib/python3.11/site-packages (3.7.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.43.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (2.32.5)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-cloud-storage) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (6.33.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.26.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f07560f-cf20-4355-bc97-93696390891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21f8ada-db1e-4335-9cd0-e08c826bad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_google_cloud_storage():\n",
    "    \"\"\"Install google-cloud-storage on the worker if it's missing.\"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "ensure_google_cloud_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc9cb0-d63f-4d98-8e35-d19dba48467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585c525e-4fd4-4d4b-af9a-a263a7264298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parallel daily CHLA(z) production using Dask-Gateway.\n",
    "\n",
    "- Searches PACE L3M Rrs DAY granules via earthaccess\n",
    "- For each granule/day:\n",
    "    * downloads Rrs\n",
    "    * runs BRT CHLA(z) prediction\n",
    "    * computes integrated/peak metrics\n",
    "    * writes a daily NetCDF locally\n",
    "    * uploads to GCS\n",
    "- Skips days that already exist in GCS unless FORCE_RERUN=True\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from google.cloud import storage\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Path to your saved ML bundle (zip) – adjust as needed\n",
    "BUNDLE_PATH = \"models/brt_chla_profiles_bundle.zip\"\n",
    "BUNDLE_FILENAME = Path(BUNDLE_PATH).name  # \"brt_chla_profiles_bundle.zip\"\n",
    "\n",
    "# GCS target\n",
    "BUCKET_NAME = \"nmfs_odp_nwfsc\"\n",
    "DESTINATION_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "# Dask-Gateway settings\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_CORES = 4\n",
    "WORKER_MEMORY = \"32GiB\"\n",
    "\n",
    "# Spatial chunking for NetCDF output\n",
    "LAT_CHUNK = 100\n",
    "LON_CHUNK = 100\n",
    "\n",
    "# Rerun control: if False, skip days that already exist in GCS\n",
    "FORCE_RERUN = False\n",
    "\n",
    "# Optional date filtering for rrs_results (None = no filter)\n",
    "START_DATE = None  # e.g. \"2024-03-01\"\n",
    "END_DATE   = None  # e.g. \"2024-04-30\"\n",
    "\n",
    "#START_DATE = \"2024-04-01\"\n",
    "#END_DATE   = \"2024-04-02\"\n",
    "\n",
    "import netrc\n",
    "import json\n",
    "\n",
    "netrc_path = os.path.expanduser(\"~/.netrc\")\n",
    "auth = netrc.netrc(netrc_path)\n",
    "login, account, password = auth.authenticators(\"urs.earthdata.nasa.gov\")\n",
    "ED_USER = login\n",
    "ED_PASS = password\n",
    "with open(\"/home/jovyan/.config/gcloud/application_default_credentials.json\") as f:\n",
    "    GCP_SA_JSON = f.read()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Helper: load ML bundle and build CHLA profile dataset\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Ensure ml_utils is available\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    import subprocess\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"-q\",\n",
    "            \"https://raw.githubusercontent.com/fish-pace/chla-z-modeling/main/ml_utils.py\",\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "import ml_utils as mu  # noqa: E402\n",
    "\n",
    "# Load the bundle once on the client side; workers will receive it via pickling\n",
    "# DELETE\n",
    "# bundle = mu.load_ml_bundle(BUNDLE_PATH)\n",
    "\n",
    "#######################\n",
    "# - Helper\n",
    "#######################\n",
    "def ensure_google_cloud_storage():\n",
    "    \"\"\"Install google-cloud-storage on the worker if it's missing.\"\"\"\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "\n",
    "\n",
    "def build_chla_profile_dataset(CHLA: xr.DataArray) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Given CHLA(time, z, lat, lon), compute derived metrics and\n",
    "    return an xr.Dataset suitable for writing to Zarr/NetCDF.\n",
    "    \"\"\"\n",
    "    # Start from CHLA's own dataset so its coords (including z_start/z_end) win\n",
    "    ds = CHLA.to_dataset(name=\"CHLA\")\n",
    "\n",
    "    # ---- Layer thickness (z dimension) ----\n",
    "    z_start = CHLA.coords.get(\"z_start\", None)\n",
    "    z_end   = CHLA.coords.get(\"z_end\", None)\n",
    "\n",
    "    if (z_start is not None) and (z_end is not None):\n",
    "        z_thick = (z_end - z_start).rename(\"z_thickness\")   # (z)\n",
    "    else:\n",
    "        # fallback: uniform layer thickness, e.g. 10 m\n",
    "        z_thick = xr.full_like(CHLA[\"z\"], 10.0).rename(\"z_thickness\")\n",
    "\n",
    "    z_center = CHLA[\"z\"]\n",
    "\n",
    "    # total CHLA in column (used for validity + center-of-mass)\n",
    "    col_total = CHLA.sum(\"z\")          # (time, lat, lon)\n",
    "    valid = col_total > 0              # True where there is some CHLA\n",
    "\n",
    "    # ---- Integrated CHLA (nominal 0–200 m; actual range = z extent) ----\n",
    "    CHLA_int = (CHLA * z_thick).sum(\"z\")\n",
    "    CHLA_int = CHLA_int.where(valid)\n",
    "    CHLA_int.name = \"CHLA_int_0_200\"\n",
    "\n",
    "    # ---- Peak value and depth (NaN-safe) ----\n",
    "    CHLA_filled = CHLA.fillna(-np.inf)\n",
    "    peak_idx = CHLA_filled.argmax(\"z\")       # (time, lat, lon) integer indices\n",
    "\n",
    "    CHLA_peak = CHLA.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak.name = \"CHLA_peak\"\n",
    "\n",
    "    CHLA_peak_depth = z_center.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak_depth.name = \"CHLA_peak_depth\"\n",
    "\n",
    "    # ---- Depth-weighted mean depth (center of mass) ----\n",
    "    num = (CHLA * z_center).sum(\"z\")\n",
    "    den = col_total\n",
    "    depth_cm = (num / den).where(valid)\n",
    "    depth_cm.name = \"CHLA_depth_center_of_mass\"\n",
    "\n",
    "    # ---- Attach derived fields to the dataset ----\n",
    "    ds[\"CHLA_int_0_200\"] = CHLA_int\n",
    "    ds[\"CHLA_peak\"] = CHLA_peak\n",
    "    ds[\"CHLA_peak_depth\"] = CHLA_peak_depth\n",
    "    ds[\"CHLA_depth_center_of_mass\"] = depth_cm\n",
    "    ds[\"z_thickness\"] = z_thick\n",
    "\n",
    "    # ---- Variable attributes ----\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"units\", \"mg m-3\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"long_name\", \"Chlorophyll-a concentration\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"standard_name\", \"mass_concentration_of_chlorophyll_a_in_sea_water\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\n",
    "        \"description\",\n",
    "        \"BRT-derived chlorophyll-a profiles from PACE hyperspectral Rrs\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_int_0_200\"].attrs.update(\n",
    "        units=\"mg m-2\",\n",
    "        long_name=\"Depth-integrated chlorophyll-a\",\n",
    "        description=(\n",
    "            \"Vertical integral of CHLA over the available depth bins \"\n",
    "            \"(nominally 0–200 m; actual range defined by z_start/z_end).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak\"].attrs.update(\n",
    "        units=\"mg m-3\",\n",
    "        long_name=\"Peak chlorophyll-a concentration in the water column\",\n",
    "        standard_name=\"mass_concentration_of_chlorophyll_a_in_sea_water\",\n",
    "        description=\"Maximum CHLA value over depth at each (time, lat, lon).\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak_depth\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Depth of peak chlorophyll-a\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth (bin center) where CHLA is maximal in the water column \"\n",
    "            \"at each (time, lat, lon).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_depth_center_of_mass\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Chlorophyll-a depth center of mass\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth of the chlorophyll-a center of mass, computed as \"\n",
    "            \"sum_z(CHLA * z) / sum_z(CHLA).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"z_thickness\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Layer thickness\",\n",
    "        description=(\n",
    "            \"Thickness of each vertical bin used for depth integration. \"\n",
    "            \"Derived from z_end - z_start when available; otherwise set to a \"\n",
    "            \"uniform nominal thickness.\"\n",
    "        ),\n",
    "    )\n",
    "    ds[\"z_thickness\"] = ds[\"z_thickness\"].expand_dims(time=ds[\"time\"])\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Worker-side function: process ONE granule/day\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def process_one_granule(\n",
    "    res,\n",
    "    lat_chunk=LAT_CHUNK,\n",
    "    lon_chunk=LON_CHUNK,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_prefix=DESTINATION_PREFIX,\n",
    "    force_rerun=FORCE_RERUN,\n",
    "    ed_username=ED_USER,\n",
    "    ed_password=ED_PASS,\n",
    "    gcp_sa_json=GCP_SA_JSON,\n",
    "    bundle_filename=BUNDLE_FILENAME,\n",
    "):\n",
    "    import os\n",
    "    import tempfile\n",
    "    import earthaccess\n",
    "    import xarray as xr\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import ml_utils as mu  # <- now workers can import this\n",
    "\n",
    "    # --- ensure google-cloud-storage is available on THIS worker ---\n",
    "    import importlib\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        importlib.import_module(\"google.cloud.storage\")\n",
    "    except ImportError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\"]\n",
    "        )\n",
    "\n",
    "    from google.cloud import storage  # now this should succeed\n",
    "\n",
    "    # --- locate the bundle file next to ml_utils.py ---\n",
    "    bundle_path = Path(mu.__file__).with_name(bundle_filename)\n",
    "    # just to be extra defensive:\n",
    "    if not bundle_path.exists():\n",
    "        raise FileNotFoundError(f\"Bundle not found at {bundle_path}\")\n",
    "\n",
    "    bundle = mu.load_ml_bundle(str(bundle_path))\n",
    "    \n",
    "    # DEBUG Load bundle on the worker from the uploaded zip file\n",
    "    #bundle = mu.load_ml_bundle(bundle_filename)\n",
    "\n",
    "    # --- EARTHACCESS AUTH VIA ENV VARS (inside worker) ---\n",
    "    if ed_username is not None and ed_password is not None:\n",
    "        os.environ[\"EARTHDATA_USERNAME\"] = ed_username\n",
    "        os.environ[\"EARTHDATA_PASSWORD\"] = ed_password\n",
    "\n",
    "    auth = earthaccess.login(strategy=\"environment\", persist=False)\n",
    "\n",
    "    # --- GCP AUTH VIA JSON TEXT (inside worker) ---\n",
    "    import uuid\n",
    "\n",
    "    cred_path = None\n",
    "    if gcp_sa_json:\n",
    "        cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\")\n",
    "        with open(cred_path, \"w\") as f:\n",
    "            f.write(gcp_sa_json)\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "\n",
    "    # -------------------------------\n",
    "    #  Normal per-day pipeline below\n",
    "    # -------------------------------\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "\n",
    "    storage_client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob_path = f\"{destination_prefix}/chla_z_{day_str}_v2.nc\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    if blob.exists() and not force_rerun:\n",
    "        msg = f\"[{day_str}] SKIP (exists at gs://{bucket_name}/{blob_path})\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    files = earthaccess.open([res], pqdm_kwargs={\"disable\": True})\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "    # DEBUG\n",
    "    # rrs_ds = rrs_ds.sel(lat=slice(40, 20), lon=slice(-70, -60) )\n",
    "\n",
    "    try:\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),\n",
    "            z_name=\"z\",\n",
    "            silent=True,\n",
    "            linear=True,\n",
    "        )\n",
    "\n",
    "        # DEBUG\n",
    "        # pred = pred.compute()\n",
    "\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "\n",
    "        # Fix chunking\n",
    "        chunks4d = (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk)\n",
    "        chunks3d = (1, lat_chunk, lon_chunk)  \n",
    "        chunks2d = (1, ds_day.sizes[\"z\"])\n",
    "        encoding = {\n",
    "            \"CHLA\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks4d},\n",
    "            \"CHLA_int_0_200\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks3d},\n",
    "            \"CHLA_peak\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks3d},\n",
    "            \"CHLA_peak_depth\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks3d},\n",
    "            \"CHLA_depth_center_of_mass\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks3d},\n",
    "            \"z_thickness\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 4, \"chunksizes\": chunks2d},\n",
    "        }\n",
    "\n",
    "        ds_day.to_netcdf(local_path, engine=\"h5netcdf\", encoding=encoding)\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "        local_path.unlink(missing_ok=True)\n",
    "\n",
    "        gcs_url = f\"gs://{bucket_name}/{blob_path}\"\n",
    "        msg = f\"[{day_str}] WROTE {gcs_url}\"\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    finally:\n",
    "        rrs_ds.close()\n",
    "        # optional: clean up the creds file\n",
    "        if cred_path is not None:\n",
    "            try:\n",
    "                os.remove(cred_path)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# DRIVER: search granules, filter, and dispatch via Dask-Gateway\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # 1. Earthaccess login on client\n",
    "    auth = earthaccess.login(strategy=\"netrc\", persist=True)\n",
    "    if not auth.authenticated:\n",
    "        raise RuntimeError(\"earthaccess login failed\")\n",
    "\n",
    "    # 2. Search PACE L3M Rrs daily granules\n",
    "    rrs_results = earthaccess.search_data(\n",
    "        short_name=\"PACE_OCI_L3M_RRS\",\n",
    "        granule_name=\"*.DAY.*.4km.nc\",\n",
    "        temporal=(START_DATE, END_DATE),\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(rrs_results)} DAY granules after date filter.\")\n",
    "    if not rrs_results:\n",
    "        print(\"Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # 4. Dask-Gateway cluster setup\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    setattr(options, \"worker_resource_allocation\", '4CPU, 30.2Gi')\n",
    "    \n",
    "    cluster = gateway.new_cluster(options)\n",
    "    cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "\n",
    "    client = cluster.get_client()\n",
    "    print(cluster)\n",
    "    print(client)\n",
    "\n",
    "    # Dashboard link (copy/paste into a browser tab)\n",
    "    print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "    # Make sure workers have needed files\n",
    "    client.upload_file(\"ml_utils.py\")\n",
    "    client.upload_file(BUNDLE_PATH)\n",
    "\n",
    "    # ensure google-cloud-storage is installed on every worker\n",
    "    client.run(ensure_google_cloud_storage)\n",
    "\n",
    "    # 5. Dispatch one task per granule\n",
    "    futures = client.map(process_one_granule, rrs_results)\n",
    "\n",
    "    # 6. Stream results as they complete (instead of blocking on gather)\n",
    "    from dask.distributed import as_completed\n",
    "\n",
    "    n = len(futures)\n",
    "    done = 0\n",
    "    errors = 0\n",
    "\n",
    "    try:\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                msg = fut.result()\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] {msg}\")\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                done += 1\n",
    "                print(f\"[{done}/{n}] ERROR: {repr(e)}\")\n",
    "                # If you want to stop on first error, uncomment:\n",
    "                # raise\n",
    "    finally:\n",
    "        print(f\"Finished. Success={done - errors}, Errors={errors}\")\n",
    "        client.close()\n",
    "        cluster.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b900c-9fe3-4cff-aef4-6aec8a7fa52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 DAY granules after date filter.\n",
      "GatewayCluster<prod.c0c526860a59442a82b1cb067f3c5a4e, status=running>\n",
      "<Client: 'tls://192.168.36.66:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.c0c526860a59442a82b1cb067f3c5a4e/status\n"
     ]
    }
   ],
   "source": [
    "# took 10 hours for 560 files\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144d4abc-3d9f-480d-bf89-8e799c431aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files: 560 first: gcs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240305.nc\n",
      "files: 559 first: gcs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240305_v2.nc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.datetime64('2025-02-06T00:00:00.000000000')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcsfs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "token = \"/home/jovyan/.config/gcloud/application_default_credentials.json\"\n",
    "fs = gcsfs.GCSFileSystem(token=token)\n",
    "# all .nc\n",
    "base = \"nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "all_nc = sorted(fs.glob(f\"{base}/chla_z_*.nc\"))\n",
    "# keep only v1 (exclude anything with _v2 anywhere in the name)\n",
    "paths_v1 = [\"gcs://\" + p for p in all_nc if \"_v2\" not in p]\n",
    "paths_v2 = [\"gcs://\" + p for p in all_nc if \"_v2\" in p]\n",
    "print(\"files:\", len(paths_v1), \"first:\", paths_v1[0])\n",
    "print(\"files:\", len(paths_v2), \"first:\", paths_v2[0])\n",
    "\n",
    "day_strs_v1 = [p.split(\"chla_z_\")[1].split(\".nc\")[0] for p in paths_v1]\n",
    "all_times_v1 = np.array(\n",
    "    pd.to_datetime(sorted(set(day_strs_v1)), format=\"%Y%m%d\"),\n",
    "    dtype=\"datetime64[ns]\"\n",
    ")\n",
    "day_strs_v2 = [p.split(\"chla_z_\")[1].split(\"_v2\")[0] for p in paths_v2]\n",
    "all_times_v2 = np.array(\n",
    "    pd.to_datetime(sorted(set(day_strs_v2)), format=\"%Y%m%d\"),\n",
    "    dtype=\"datetime64[ns]\"\n",
    ")\n",
    "missing = [x for x in all_times_v1 if x not in all_times_v2]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd12074-148e-4d94-8a01-ab96cc9dd8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50601fdb-1f7c-475b-908a-58ad6f40f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 DAY granules after date filter.\n",
      "GatewayCluster<prod.6cf49325472e4ac4a66bd3af73c1a4bc, status=running>\n",
      "<Client: 'tls://192.168.53.134:8786' processes=0 threads=0, memory=0 B>\n",
      "Dask dashboard: /services/dask-gateway/clusters/prod.6cf49325472e4ac4a66bd3af73c1a4bc/status\n"
     ]
    }
   ],
   "source": [
    "# need to rerun a few days\n",
    "START_DATE = '20250206'\n",
    "END_DATE   = '20250206'\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89769d4-9f8d-439d-a575-87d930f2d0d6",
   "metadata": {},
   "source": [
    "# Process Zarr\n",
    "\n",
    "Should have baked this into the first pipeline. Alas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b19974d-2fdd-49e1-ae3e-6a3949f863a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chla_zarr_worker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chla_zarr_worker.py\n",
    "import os, uuid, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import zarr\n",
    "\n",
    "def normalize_time(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if \"time\" in ds.coords:\n",
    "        ds = ds.assign_coords(time=ds[\"time\"].astype(\"datetime64[ns]\"))\n",
    "    return ds\n",
    "\n",
    "def make_time_only_for_region(ds: xr.Dataset) -> xr.Dataset:\n",
    "    time_vars = [v for v, da in ds.data_vars.items() if \"time\" in da.dims]\n",
    "    if not time_vars:\n",
    "        raise ValueError(\"No time-dependent variables found for region write.\")\n",
    "    ds_time = ds[time_vars]\n",
    "    drop_coord_vars = [c for c in list(ds_time.coords) if c != \"time\"]\n",
    "    if drop_coord_vars:\n",
    "        ds_time = ds_time.drop_vars(drop_coord_vars)\n",
    "    ds_time = ds_time.assign_coords(time=ds_time[\"time\"].astype(\"datetime64[ns]\"))\n",
    "    return ds_time\n",
    "\n",
    "def chunk_spec(ds: xr.Dataset, lat_chunk: int, lon_chunk: int) -> dict:\n",
    "    spec = {}\n",
    "    if \"time\" in ds.dims: spec[\"time\"] = 1\n",
    "    if \"z\" in ds.dims:    spec[\"z\"] = ds.sizes[\"z\"]\n",
    "    if \"lat\" in ds.dims:  spec[\"lat\"] = lat_chunk\n",
    "    if \"lon\" in ds.dims:  spec[\"lon\"] = lon_chunk\n",
    "    return spec\n",
    "\n",
    "def assert_dimension_names_present(mapper, ds: xr.Dataset) -> None:\n",
    "    root = zarr.open_group(mapper, mode=\"r\")\n",
    "    def has_dn(a):\n",
    "        return bool(getattr(a.metadata, \"dimension_names\", None))\n",
    "    if \"time\" not in root or not has_dn(root[\"time\"]):\n",
    "        raise KeyError(\"Store missing v3 dimension_names for 'time' (recreate store at fresh path).\")\n",
    "    for v in ds.data_vars:\n",
    "        if v not in root or not has_dn(root[v]):\n",
    "            raise KeyError(f\"Store missing v3 dimension_names for {v!r} (recreate store at fresh path).\")\n",
    "\n",
    "def write_one_day_region(gcs_url: str, time_index: int, zarr_path: str, gcp_sa_json: str,\n",
    "                         lat_chunk: int, lon_chunk: int) -> str:\n",
    "    # auth on worker\n",
    "    cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_worker_{uuid.uuid4().hex}.json\")\n",
    "    with open(cred_path, \"w\") as f:\n",
    "        f.write(gcp_sa_json)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "\n",
    "    try:\n",
    "        fs = gcsfs.GCSFileSystem(token=\"google_default\")\n",
    "        mapper = fs.get_mapper(zarr_path)\n",
    "\n",
    "        tmp_dir = Path(\"/tmp/chla_nc_to_zarr_workers\")\n",
    "        tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        local_nc = tmp_dir / Path(gcs_url).name\n",
    "\n",
    "        fs.get(gcs_url, str(local_nc))\n",
    "\n",
    "        ds = xr.open_dataset(local_nc, engine=\"h5netcdf\")\n",
    "        try:\n",
    "            ds = normalize_time(ds)\n",
    "            ds_time = make_time_only_for_region(ds).chunk(chunk_spec(ds, lat_chunk, lon_chunk))\n",
    "\n",
    "            assert_dimension_names_present(mapper, ds_time)\n",
    "\n",
    "            region = {\"time\": slice(time_index, time_index + 1)}\n",
    "            ds_time.to_zarr(mapper, mode=\"r+\", region=region, consolidated=False, zarr_version=3)\n",
    "        finally:\n",
    "            ds.close()\n",
    "            local_nc.unlink(missing_ok=True)\n",
    "\n",
    "        return f\"OK {Path(gcs_url).name} -> time_index={time_index}\"\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1bc9f4b-d4f1-4268-9e9f-2ed367c45ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Gateway.__del__ at 0x7f5af1c756c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/dask_gateway/client.py\", line 380, in __del__\n",
      "    self.close()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/dask_gateway/client.py\", line 353, in close\n",
      "    elif self.loop.asyncio_loop.is_running():\n",
      "         ^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/dask_gateway/client.py\", line 330, in loop\n",
      "    return self._loop_runner.loop\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/distributed/utils.py\", line 648, in loop\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Accessing the loop property while the loop is not running is not supported\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import zarr\n",
    "\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "import chla_zarr_worker as wz  # <-- your uploaded module\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG (edit as needed)\n",
    "# =============================================================================\n",
    "TOKEN_PATH = \"/home/jovyan/.config/gcloud/application_default_credentials.json\"\n",
    "\n",
    "BUCKET = \"nmfs_odp_nwfsc\"\n",
    "NETCDF_PREFIX = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "NETCDF_PATTERN = f\"{BUCKET}/{NETCDF_PREFIX}/chla_z_*.nc\"\n",
    "\n",
    "# IMPORTANT: start with a fresh v3 path\n",
    "ZARR_PATH = f\"gcs://{BUCKET}/CB/fish-pace-datasets/chla-z/zarr_v12\"\n",
    "\n",
    "LAT_CHUNK = 128\n",
    "LON_CHUNK = 128\n",
    "\n",
    "MIN_WORKERS = 4\n",
    "MAX_WORKERS = 12\n",
    "WORKER_RESOURCE = \"4CPU, 30.2Gi\"\n",
    "\n",
    "# =============================================================================\n",
    "# LISTING + TIME AXIS\n",
    "# =============================================================================\n",
    "_date_re = re.compile(r\"chla_z_(\\d{8})\\.nc$\")\n",
    "\n",
    "\n",
    "def date_from_url(gcs_url: str) -> pd.Timestamp:\n",
    "    m = _date_re.search(gcs_url)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse date from: {gcs_url}\")\n",
    "    return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "\n",
    "def list_netcdf_urls(fs: gcsfs.GCSFileSystem) -> list[str]:\n",
    "    paths = sorted(fs.glob(NETCDF_PATTERN))\n",
    "    urls = [\"gcs://\" + p for p in paths]\n",
    "    return sorted(urls, key=date_from_url)\n",
    "\n",
    "\n",
    "def build_time_index(urls: list[str]) -> pd.DatetimeIndex:\n",
    "    times = pd.to_datetime([date_from_url(u) for u in urls]).astype(\"datetime64[ns]\")\n",
    "    return pd.DatetimeIndex(times, name=\"time\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AUTH HELPERS\n",
    "# =============================================================================\n",
    "def read_sa_json(token_path: str = TOKEN_PATH) -> str:\n",
    "    with open(token_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def make_gcsfs_with_sa_json(gcp_sa_json: str) -> tuple[gcsfs.GCSFileSystem, str]:\n",
    "    cred_path = os.path.join(tempfile.gettempdir(), f\"gcp_sa_{uuid.uuid4().hex}.json\")\n",
    "    with open(cred_path, \"w\") as f:\n",
    "        f.write(gcp_sa_json)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "    fs = gcsfs.GCSFileSystem(token=\"google_default\")\n",
    "    return fs, cred_path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ZARR v3 TEMPLATE (dimension_names required)\n",
    "# =============================================================================\n",
    "def ensure_store_is_empty(fs: gcsfs.GCSFileSystem, zarr_path: str) -> None:\n",
    "    if not zarr_path.startswith(\"gcs://\"):\n",
    "        raise ValueError(\"Expected zarr_path like gcs://bucket/prefix\")\n",
    "    no_scheme = zarr_path[len(\"gcs://\") :]\n",
    "    try:\n",
    "        existing = fs.ls(no_scheme)\n",
    "    except FileNotFoundError:\n",
    "        existing = []\n",
    "    if existing:\n",
    "        raise RuntimeError(\n",
    "            f\"Target ZARR_PATH is not empty:\\n  {zarr_path}\\n\"\n",
    "            \"Pick a NEW ZARR_PATH or delete the existing prefix before running.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def normalize_time(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if \"time\" in ds.coords:\n",
    "        ds = ds.assign_coords(time=ds[\"time\"].astype(\"datetime64[ns]\"))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_zarr_v3_template_direct(\n",
    "    gcp_sa_json: str,\n",
    "    sample_url: str,\n",
    "    times: pd.DatetimeIndex,\n",
    "    zarr_path: str,\n",
    "    lat_chunk: int,\n",
    "    lon_chunk: int,\n",
    ") -> None:\n",
    "    fs, cred_path = make_gcsfs_with_sa_json(gcp_sa_json)\n",
    "    try:\n",
    "        ensure_store_is_empty(fs, zarr_path)\n",
    "\n",
    "        tmp_dir = Path(\"/tmp/chla_zarr_template_v3\")\n",
    "        tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        local_nc = tmp_dir / Path(sample_url).name\n",
    "        fs.get(sample_url, str(local_nc))\n",
    "\n",
    "        ds0 = xr.open_dataset(local_nc, engine=\"h5netcdf\")\n",
    "        try:\n",
    "            ds0 = normalize_time(ds0)\n",
    "            nt = len(times)\n",
    "\n",
    "            mapper = fs.get_mapper(zarr_path)\n",
    "            root = zarr.group(store=mapper, overwrite=True, zarr_format=3)\n",
    "\n",
    "            # time coordinate (int64 ns)\n",
    "            time_int = times.values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "            root.create_array(\n",
    "                name=\"time\",\n",
    "                shape=(nt,),\n",
    "                chunks=(min(nt, 1024),),\n",
    "                dtype=\"int64\",\n",
    "                dimension_names=(\"time\",),\n",
    "                overwrite=True,\n",
    "            )\n",
    "            root[\"time\"][:] = time_int\n",
    "\n",
    "            # coords: use data=..., and DO NOT pass dtype/shape with data\n",
    "            for cname in [\"lat\", \"lon\", \"z\", \"z_start\", \"z_end\"]:\n",
    "                if cname in ds0.coords:\n",
    "                    vals = np.asarray(ds0[cname].values)\n",
    "                    dims = tuple(ds0[cname].dims)\n",
    "                    root.create_array(\n",
    "                        name=cname,\n",
    "                        data=vals,\n",
    "                        chunks=vals.shape,\n",
    "                        dimension_names=dims,\n",
    "                        overwrite=True,\n",
    "                    )\n",
    "\n",
    "            # data vars: create empty arrays for region writes\n",
    "            for v in ds0.data_vars:\n",
    "                da0 = ds0[v]\n",
    "                dims = tuple(da0.dims)\n",
    "\n",
    "                shape: list[int] = []\n",
    "                chunks: list[int] = []\n",
    "                for d in dims:\n",
    "                    if d == \"time\":\n",
    "                        shape.append(nt); chunks.append(1)\n",
    "                    elif d == \"z\":\n",
    "                        shape.append(ds0.sizes[\"z\"]); chunks.append(ds0.sizes[\"z\"])\n",
    "                    elif d == \"lat\":\n",
    "                        shape.append(ds0.sizes[\"lat\"]); chunks.append(lat_chunk)\n",
    "                    elif d == \"lon\":\n",
    "                        shape.append(ds0.sizes[\"lon\"]); chunks.append(lon_chunk)\n",
    "                    else:\n",
    "                        shape.append(ds0.sizes[d]); chunks.append(ds0.sizes[d])\n",
    "\n",
    "                fill_value = float(\"nan\") if np.issubdtype(da0.dtype, np.floating) else 0\n",
    "\n",
    "                root.create_array(\n",
    "                    name=v,\n",
    "                    shape=tuple(shape),\n",
    "                    chunks=tuple(chunks),\n",
    "                    dtype=da0.dtype,\n",
    "                    fill_value=fill_value,\n",
    "                    dimension_names=dims,\n",
    "                    overwrite=True,\n",
    "                )\n",
    "\n",
    "        finally:\n",
    "            ds0.close()\n",
    "            local_nc.unlink(missing_ok=True)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "def zarr_main():\n",
    "    gcp_sa_json = read_sa_json(TOKEN_PATH)\n",
    "\n",
    "    # list urls on client\n",
    "    fs, cred_path = make_gcsfs_with_sa_json(gcp_sa_json)\n",
    "    try:\n",
    "        urls = list_netcdf_urls(fs)\n",
    "        if not urls:\n",
    "            raise RuntimeError(\"No NetCDF files found with pattern: \" + NETCDF_PATTERN)\n",
    "        print(\"nfiles:\", len(urls), \"first:\", urls[0])\n",
    "        times = build_time_index(urls)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(cred_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    print(f\"Creating Zarr v3 template at: {ZARR_PATH}\")\n",
    "    create_zarr_v3_template_direct(\n",
    "        gcp_sa_json=gcp_sa_json,\n",
    "        sample_url=urls[0],\n",
    "        times=times,\n",
    "        zarr_path=ZARR_PATH,\n",
    "        lat_chunk=LAT_CHUNK,\n",
    "        lon_chunk=LON_CHUNK,\n",
    "    )\n",
    "    print(\"Template created:\", ZARR_PATH)\n",
    "\n",
    "    # Dask-Gateway cluster\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    setattr(options, \"worker_resource_allocation\", WORKER_RESOURCE)\n",
    "\n",
    "    cluster = gateway.new_cluster(options)\n",
    "    cluster.adapt(minimum=MIN_WORKERS, maximum=MAX_WORKERS)\n",
    "    client = cluster.get_client()\n",
    "\n",
    "    print(cluster)\n",
    "    print(client)\n",
    "    print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "    # ensure workers have your module\n",
    "    client.upload_file(\"chla_zarr_worker.py\")\n",
    "    client.run(lambda: __import__(\"chla_zarr_worker\").__name__)\n",
    "\n",
    "    # (optional) broadcast JSON once instead of embedding in every task payload\n",
    "    gcp_sa_json_fut = client.scatter(gcp_sa_json, broadcast=True)\n",
    "\n",
    "    futures = []\n",
    "    fut_to_info = {}\n",
    "    for idx, url in enumerate(urls):\n",
    "        fut = client.submit(\n",
    "            wz.write_one_day_region,\n",
    "            url,\n",
    "            idx,\n",
    "            ZARR_PATH,\n",
    "            gcp_sa_json_fut,  # <- scattered once\n",
    "            LAT_CHUNK,\n",
    "            LON_CHUNK,\n",
    "            pure=False,\n",
    "        )\n",
    "        futures.append(fut)\n",
    "        fut_to_info[fut.key] = (idx, url)\n",
    "\n",
    "    n = len(futures)\n",
    "    done = 0\n",
    "    errors = 0\n",
    "\n",
    "    try:\n",
    "        for fut in as_completed(futures):\n",
    "            done += 1\n",
    "            idx, url = fut_to_info.get(fut.key, (None, None))\n",
    "            try:\n",
    "                msg = fut.result()\n",
    "                print(f\"[{done}/{n}] {msg}\")\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                print(f\"[{done}/{n}] ERROR on idx={idx} url={url}\")\n",
    "                print(f\"Exception: {type(e).__name__}: {e}\")\n",
    "\n",
    "        print(f\"Finished region writes. Success={n - errors}, Errors={errors}\")\n",
    "\n",
    "    finally:\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a9bb9-4d3a-4623-93b3-9ec18dfc2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d509d3-62bc-4159-8d8a-4983f334ce31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2f69a-d9da-4185-acd8-76135736bd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
