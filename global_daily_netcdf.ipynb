{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe6c8b9-2581-4dde-9447-072103be47bd",
   "metadata": {},
   "source": [
    "# Create daily global netcdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c0e36d3-cba1-439f-a35f-d5c9785385b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core data handling and plotting libraries ---\n",
    "import earthaccess\n",
    "import xarray as xr       # for working with labeled multi-dimensional arrays\n",
    "import numpy as np        # for numerical operations on arrays\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # for creating plots\n",
    "import cartopy.crs as ccrs\n",
    "import sklearn\n",
    "# --- Custom python functions ---\n",
    "import os, importlib\n",
    "# Looks to see if you have the file already and if not, downloads from GitHub\n",
    "if not os.path.exists(\"ml_utils.py\"):\n",
    "    !wget -q https://raw.githubusercontent.com/fish-pace/2025-tutorials/main/ml_utils.py\n",
    "\n",
    "import ml_utils as mu\n",
    "importlib.reload(mu)\n",
    "# core stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.ndimage import uniform_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a0d52-a78d-4efd-9241-46ba53689480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c288a3-9842-4c27-9030-55adb0e18238",
   "metadata": {},
   "source": [
    "## Read in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17bd33ba-7d82-472f-8b07-5ac9ee33e8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded ML bundle from: models/brt_chla_profiles_bundle.zip\n",
      "  model_kind : pickle\n",
      "  model_type : collection (dict), n_submodels=20\n",
      "  example key: CHLA_0_10\n",
      "  target     : log10_CHLA_A_B depth bins\n",
      "  features   : 174 columns\n",
      "  train/test : 4408 / 1102 rows\n",
      "  dataset    : 5510 rows stored in bundle\n",
      "\n",
      "Usage example (Python):\n",
      "  bundle = load_ml_bundle('path/to/bundle.zip')\n",
      "  # Predict using helper 'predict_all_depths_for_day'\n",
      "  # Example: predict all depths for one day from a BRF dataset R\n",
      "  pred = bundle.predict(\n",
      "      R_dataset,                  # xr.DataArray/xr.Dataset with lat/lon + predictors\n",
      "      brt_models=bundle.model,    # dict of models by depth bin\n",
      "      feature_cols=bundle.meta['feature_cols'],\n",
      "      consts={'solar_hour': 12.0, 'type': 1},\n",
      "  )  # -> e.g. CHLA(time?, z, lat, lon)\n",
      "\n",
      "  # Plot using helper 'make_plot_pred_map'\n",
      "  fig, ax = bundle.plot(pred_da, pred_label='Prediction')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Load model\n",
    "bundle = mu.load_ml_bundle(\"models/brt_chla_profiles_bundle.zip\")\n",
    "brt_models = bundle.model\n",
    "meta = bundle.meta\n",
    "rrs_cols = meta[\"rrs_cols\"]\n",
    "chl_cols = meta[\"y_col\"]\n",
    "extra = meta[\"extra_cols\"]\n",
    "dataset = bundle.data[\"dataset\"]\n",
    "train_idx = bundle.data[\"train_idx\"]\n",
    "test_idx = bundle.data[\"test_idx\"]\n",
    "X_train = bundle.data[\"X_train\"]\n",
    "X_test = bundle.data[\"X_test\"]\n",
    "y_train_all = bundle.data[\"y_train\"]\n",
    "y_test_all = bundle.data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b51f99-9948-4bb2-9f86-7635f0a8d6bb",
   "metadata": {},
   "source": [
    "## Create a dataset with our derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b2892195-d70e-4f64-b730-a74a28941052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def build_chla_profile_dataset(CHLA: xr.DataArray) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Given CHLA(time, z, lat, lon), compute derived metrics and\n",
    "    return an xr.Dataset suitable for writing to Zarr/NetCDF.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start from CHLA's own dataset so its coords (including z_start/z_end) win\n",
    "    ds = CHLA.to_dataset(name=\"CHLA\")\n",
    "\n",
    "    # ---- Layer thickness (z dimension) ----\n",
    "    z_start = CHLA.coords.get(\"z_start\", None)\n",
    "    z_end   = CHLA.coords.get(\"z_end\", None)\n",
    "\n",
    "    if (z_start is not None) and (z_end is not None):\n",
    "        z_thick = (z_end - z_start).rename(\"z_thickness\")   # (z)\n",
    "    else:\n",
    "        # fallback: uniform layer thickness, e.g. 10 m\n",
    "        z_thick = xr.full_like(CHLA[\"z\"], 10.0).rename(\"z_thickness\")\n",
    "\n",
    "    z_center = CHLA[\"z\"]\n",
    "\n",
    "    # total CHLA in column (used for validity + center-of-mass)\n",
    "    col_total = CHLA.sum(\"z\")          # (time, lat, lon)\n",
    "    valid = col_total > 0              # True where there is some CHLA\n",
    "\n",
    "    # ---- Integrated CHLA (nominal 0–200 m; actual range = z extent) ----\n",
    "    CHLA_int = (CHLA * z_thick).sum(\"z\")\n",
    "    CHLA_int = CHLA_int.where(valid)\n",
    "    CHLA_int.name = \"CHLA_int_0_200\"\n",
    "\n",
    "    # ---- Peak value and depth (NaN-safe) ----\n",
    "    CHLA_filled = CHLA.fillna(-np.inf)\n",
    "    peak_idx = CHLA_filled.argmax(\"z\")       # (time, lat, lon) integer indices\n",
    "\n",
    "    CHLA_peak = CHLA.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak.name = \"CHLA_peak\"\n",
    "\n",
    "    CHLA_peak_depth = z_center.isel(z=peak_idx).where(valid)\n",
    "    CHLA_peak_depth.name = \"CHLA_peak_depth\"\n",
    "\n",
    "    # ---- Depth-weighted mean depth (center of mass) ----\n",
    "    num = (CHLA * z_center).sum(\"z\")\n",
    "    den = col_total\n",
    "    depth_cm = (num / den).where(valid)\n",
    "    depth_cm.name = \"CHLA_depth_center_of_mass\"\n",
    "\n",
    "    # ---- Attach derived fields to the dataset ----\n",
    "    ds[\"CHLA_int_0_200\"] = CHLA_int\n",
    "    ds[\"CHLA_peak\"] = CHLA_peak\n",
    "    ds[\"CHLA_peak_depth\"] = CHLA_peak_depth\n",
    "    ds[\"CHLA_depth_center_of_mass\"] = depth_cm\n",
    "    ds[\"z_thickness\"] = z_thick\n",
    "\n",
    "    # ---- Variable attributes ----\n",
    "    # CHLA itself should already have attrs from the prediction step\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"units\", \"mg m-3\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\"long_name\", \"Chlorophyll-a concentration\")\n",
    "    ds[\"CHLA\"].attrs.setdefault(\n",
    "        \"description\",\n",
    "        \"BRT-derived chlorophyll-a profiles from PACE hyperspectral Rrs\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_int_0_200\"].attrs.update(\n",
    "        units=\"mg m-2\",\n",
    "        long_name=\"Depth-integrated chlorophyll-a\",\n",
    "        description=(\n",
    "            \"Vertical integral of CHLA over the available depth bins \"\n",
    "            \"(nominally 0–200 m; actual range defined by z_start/z_end).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak\"].attrs.update(\n",
    "        units=\"mg m-3\",\n",
    "        long_name=\"Peak chlorophyll-a concentration in the water column\",\n",
    "        description=\"Maximum CHLA value over depth at each (time, lat, lon).\",\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_peak_depth\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Depth of peak chlorophyll-a\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth (bin center) where CHLA is maximal in the water column \"\n",
    "            \"at each (time, lat, lon).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"CHLA_depth_center_of_mass\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Chlorophyll-a depth center of mass\",\n",
    "        positive=\"down\",\n",
    "        description=(\n",
    "            \"Depth of the chlorophyll-a center of mass, computed as \"\n",
    "            \"sum_z(CHLA * z) / sum_z(CHLA).\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ds[\"z_thickness\"].attrs.update(\n",
    "        units=\"m\",\n",
    "        long_name=\"Layer thickness\",\n",
    "        description=(\n",
    "            \"Thickness of each vertical bin used for depth integration. \"\n",
    "            \"Derived from z_end - z_start when available; otherwise set to a \"\n",
    "            \"uniform nominal thickness.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # You can still add global ds.attrs later in your pipeline\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3155ea6-e5cf-47fd-af42-83c48d8f6664",
   "metadata": {},
   "source": [
    "## Test pipeline bits\n",
    "\n",
    "Get data and make prediction, build dataset. Save a test file to gcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cd45edc-f94f-4c75-b44b-64cfd7616640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0 of 480\n",
      "Starting 100 of 480\n",
      "Starting 200 of 480\n",
      "Starting 300 of 480\n",
      "Starting 400 of 480\n",
      "Starting wrapping\n",
      "Adding coords\n"
     ]
    }
   ],
   "source": [
    "# Get some Rrs data\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "\n",
    "day = \"2024-07-08\"\n",
    "\n",
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)\n",
    "\n",
    "# Get Rrs\n",
    "rrs_results = earthaccess.search_data(\n",
    "    short_name = \"PACE_OCI_L3M_RRS\",\n",
    "    temporal = (day, day),\n",
    "    granule_name=\"*.DAY.*.4km.nc\"\n",
    ")\n",
    "f = earthaccess.open(rrs_results[0:1], pqdm_kwargs={\"disable\": True})\n",
    "rrs_ds = xr.open_dataset(f[0])\n",
    "\n",
    "lat_min, lat_max = 20, 40\n",
    "lon_min, lon_max = -70, -60\n",
    "# Get Rrs for that box\n",
    "R = rrs_ds[\"Rrs\"].sel(\n",
    "    lat=slice(lat_max, lat_min),   # decreasing lat coord: max→min\n",
    "    lon=slice(lon_min, lon_max)\n",
    ")\n",
    "pred = bundle.predict(\n",
    "      R,                  # xr.DataArray/xr.Dataset with lat/lon + predictors\n",
    "      brt_models=bundle.model,    # dict of models by depth bin\n",
    "      feature_cols=bundle.meta['feature_cols'],\n",
    "      consts={'solar_hour': 0, 'type': 1},\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cfbc1-f1e7-4776-b88e-479e2b3ab17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ds = build_chla_profile_dataset(pred)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb252892-1a4e-42f2-b863-e3983c60dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded test.pdf → gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/test.pdf\n"
     ]
    }
   ],
   "source": [
    "# Test saving file to the bucket\n",
    "# stop annoying warnings\n",
    "# https://console.cloud.google.com/storage/browser/nmfs_odp_nwfsc/CB/fish-pace-datasets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Your application has authenticated using end user credentials\")\n",
    "\n",
    "from google.cloud import storage\n",
    "from pathlib import Path\n",
    "\n",
    "# === Set these ===\n",
    "bucket_name = \"nmfs_odp_nwfsc\"\n",
    "\n",
    "# Create client and bucket\n",
    "client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Set the file you want to test with\n",
    "test_file = Path(\"test.pdf\")  # change this if using a different file\n",
    "destination_prefix = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "# Create blob and upload\n",
    "blob_path = f\"{destination_prefix}/{test_file.name}\"\n",
    "blob = bucket.blob(blob_path)\n",
    "blob.upload_from_filename(str(test_file))\n",
    "\n",
    "print(f\"Uploaded {test_file.name} → gs://{bucket_name}/{blob_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6ef4f-4d93-426d-b8a4-473fb5df1ff6",
   "metadata": {},
   "source": [
    "## Full pipeline\n",
    "\n",
    "* dataset to netcdf in google bucket\n",
    "* Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "46416e46-1982-4ec4-962e-24d95b8a3892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Rrs data\n",
    "import earthaccess\n",
    "import xarray as xr\n",
    "\n",
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)\n",
    "\n",
    "# Get Rrs\n",
    "rrs_results = earthaccess.search_data(\n",
    "    short_name = \"PACE_OCI_L3M_RRS\",\n",
    "    granule_name=\"*.DAY.*.4km.nc\"\n",
    ")\n",
    "len(rrs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e589d0e3-e16d-4dbc-b5b0-6bf4661557ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20240305...\n",
      "Wrote /tmp/chla_z_20240305.nc → gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240305.nc\n",
      "Processing 20240306...\n",
      "Wrote /tmp/chla_z_20240306.nc → gs://nmfs_odp_nwfsc/CB/fish-pace-datasets/chla-z/netcdf/chla_z_20240306.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import earthaccess\n",
    "from google.cloud import storage\n",
    "import tempfile\n",
    "\n",
    "# === Set these ===\n",
    "bucket_name = \"nmfs_odp_nwfsc\"\n",
    "# Create client and bucket\n",
    "client = storage.Client(project=\"noaa-gcs-public-data\")\n",
    "bucket = client.bucket(bucket_name)\n",
    "destination_prefix = \"CB/fish-pace-datasets/chla-z/netcdf\"\n",
    "\n",
    "lat_chunk = 100\n",
    "lon_chunk = 100\n",
    "\n",
    "# just test on first two granules\n",
    "for res in rrs_results:\n",
    "    # day as ISO string from UMM\n",
    "    day_iso = res[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
    "    day = pd.to_datetime(day_iso)          # Timestamp\n",
    "    day_str = day.strftime(\"%Y%m%d\")\n",
    "    print(f\"Processing {day_str}...\")\n",
    "\n",
    "    # auth per granule (you can optimize this later if you want)\n",
    "    auth = earthaccess.login()\n",
    "    files = earthaccess.open([res], pqdm_kwargs={\"disable\": True})\n",
    "\n",
    "    # open dataset\n",
    "    rrs_ds = xr.open_dataset(files[0])\n",
    "    # debugging line\n",
    "    #rrs_ds = rrs_ds.sel(lat=slice(40, 20), lon=slice(-70, -60) )\n",
    "\n",
    "    try:\n",
    "        # Rrs for that day\n",
    "        # (if the file has only one time, squeeze; otherwise select)\n",
    "        if \"time\" in rrs_ds.dims:\n",
    "            R = rrs_ds[\"Rrs\"].sel(time=day).squeeze(\"time\")\n",
    "        else:\n",
    "            R = rrs_ds[\"Rrs\"]\n",
    "        R = R.transpose(\"lat\", \"lon\", \"wavelength\")\n",
    "\n",
    "        # BRT predictions for all depths\n",
    "        pred = bundle.predict(\n",
    "            R,\n",
    "            brt_models=bundle.model,\n",
    "            feature_cols=bundle.meta[\"feature_cols\"],\n",
    "            consts={\"solar_hour\": 0, \"type\": 1},\n",
    "            chunk_size_lat=100,\n",
    "            time=day.to_datetime64(),   # time coord length 1\n",
    "            z_name=\"z\",\n",
    "            silent=True,\n",
    "        )  # (time=1, z, lat, lon), float32\n",
    "\n",
    "        # Build full dataset with integrated/peak metrics\n",
    "        ds_day = build_chla_profile_dataset(pred)\n",
    "\n",
    "        # Add metadata\n",
    "        ds_day[\"CHLA\"].attrs.update(\n",
    "            units=\"mg m-3\",\n",
    "            long_name=\"Chlorophyll-a concentration\",\n",
    "            description=\"BRT-derived CHLA profiles from PACE hyperspectral Rrs\",\n",
    "        )\n",
    "        ds_day[\"z\"].attrs.update(units=\"m\", long_name=\"depth (bin center)\")\n",
    "        ds_day[\"lat\"].attrs.update(units=\"degrees_north\")\n",
    "        ds_day[\"lon\"].attrs.update(units=\"degrees_east\")\n",
    "        ds_day.attrs[\"source\"] = \"BRT model trained on BGC-Argo + OOI matchups\"\n",
    "        ds_day.attrs[\"model_bundle\"] = Path(\"path/to/bundle.zip\").name\n",
    "\n",
    "        # Chunking for NetCDF\n",
    "        encoding = {\n",
    "            \"CHLA\": {\n",
    "                \"dtype\": \"float32\",\n",
    "                \"zlib\": True,\n",
    "                \"complevel\": 4,\n",
    "                \"chunksizes\": (1, ds_day.sizes[\"z\"], lat_chunk, lon_chunk),\n",
    "            }\n",
    "            # you can add encodings for derived vars later if desired\n",
    "        }\n",
    "\n",
    "        # 4. Write to a local temporary NetCDF, then upload to GCS\n",
    "        tmp_dir = Path(tempfile.gettempdir())\n",
    "        local_path = tmp_dir / f\"chla_z_{day_str}.nc\"\n",
    "        \n",
    "        ds_day.to_netcdf(\n",
    "            local_path,\n",
    "            engine=\"h5netcdf\",\n",
    "            encoding=encoding,\n",
    "        )\n",
    "        \n",
    "        blob_path = f\"{destination_prefix}/chla_z_{day_str}.nc\"\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(str(local_path))\n",
    "        \n",
    "        print(f\"Wrote {local_path} → gs://{bucket_name}/{blob_path}\")\n",
    "        \n",
    "    finally:\n",
    "        rrs_ds.close()\n",
    "        del ds_day, pred, R, rrs_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e20d480-137e-455e-9ace-23018f6b5669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29ea58-811f-4ce6-b165-426e8de24ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
